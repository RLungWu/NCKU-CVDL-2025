"""
PPO (Proximal Policy Optimization) è¨“ç·´ Super Mario Bros
ä½¿ç”¨å®Œæ•´çš„æ™ºæ…§çå‹µç³»çµ± + å¹³è¡Œç’°å¢ƒ

æ”¹é€²ç‰ˆæœ¬ï¼š
- ä½¿ç”¨ reward.py çš„æ™ºæ…§çå‹µç³»çµ±
- å¤šå€‹å¹³è¡Œç’°å¢ƒåŒæ™‚è¨“ç·´
- å„ªåŒ–çš„è¶…åƒæ•¸
"""
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from tqdm import tqdm

import gym
import gym_super_mario_bros
from gym.wrappers import StepAPICompatibility
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT

from utils import preprocess_frame
from reward import calculate_smart_reward, reset_max_x, reset_enemy_tracking, reset_hole_tracking

# ========== PPO é…ç½® (å„ªåŒ–ç‰ˆ) ===========
NUM_ENVS = 8                    # å¹³è¡Œç’°å¢ƒæ•¸é‡ (å¢åŠ ä»¥æ”¶é›†æ›´å¤šç¶“é©—)
LEARNING_RATE = 2.5e-4          # å­¸ç¿’ç‡
GAMMA = 0.99                    # æŠ˜æ‰£å› å­
GAE_LAMBDA = 0.95               # GAE åƒæ•¸
CLIP_EPSILON = 0.1              # PPO clipping åƒæ•¸ (é™ä½ä»¥æé«˜ç©©å®šæ€§)
ENTROPY_COEF = 0.02             # ç†µæ­£å‰‡åŒ–ä¿‚æ•¸ (å¢åŠ ä»¥é¼“å‹µæ¢ç´¢)
VALUE_COEF = 0.5                # åƒ¹å€¼å‡½æ•¸æå¤±ä¿‚æ•¸
MAX_GRAD_NORM = 0.5             # æ¢¯åº¦è£å‰ª
PPO_EPOCHS = 4                  # æ¯æ¬¡æ›´æ–°çš„ epoch æ•¸
BATCH_SIZE = 256                # Mini-batch å¤§å° (å¢åŠ )
ROLLOUT_STEPS = 256             # æ¯æ¬¡æ”¶é›†çš„æ­¥æ•¸ (å¢åŠ )
TOTAL_TIMESTEPS = 2000000       # ç¸½è¨“ç·´æ­¥æ•¸ (å¢åŠ )
USE_SMART_REWARD = True         # ä½¿ç”¨æ™ºæ…§çå‹µç³»çµ±
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ========== Actor-Critic ç¶²è·¯ ===========
class ActorCritic(nn.Module):
    """
    Actor-Critic ç¶²è·¯
    å…±äº«ç‰¹å¾µæå–å±¤ï¼Œåˆ†åˆ¥è¼¸å‡ºç­–ç•¥å’Œåƒ¹å€¼
    """
    def __init__(self, obs_shape, n_actions):
        super(ActorCritic, self).__init__()
        
        # å…±äº«å·ç©å±¤
        self.conv = nn.Sequential(
            nn.Conv2d(obs_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
        )
        
        # è¨ˆç®—å·ç©è¼¸å‡ºå¤§å°
        conv_out_size = self._get_conv_out(obs_shape)
        
        # å…±äº«å…¨é€£æ¥å±¤
        self.fc = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
        )
        
        # Actor (ç­–ç•¥) è¼¸å‡º
        self.actor = nn.Linear(512, n_actions)
        
        # Critic (åƒ¹å€¼) è¼¸å‡º
        self.critic = nn.Linear(512, 1)
        
    def _get_conv_out(self, shape):
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))
    
    def forward(self, x):
        x = x / 255.0  # æ­£è¦åŒ–
        conv_out = self.conv(x)
        fc_out = self.fc(conv_out)
        
        # ç­–ç•¥ (log probabilities)
        policy = self.actor(fc_out)
        # åƒ¹å€¼
        value = self.critic(fc_out)
        
        return policy, value
    
    def get_action(self, state):
        """é¸æ“‡å‹•ä½œ"""
        policy, value = self.forward(state)
        probs = torch.softmax(policy, dim=-1)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action, log_prob, value.squeeze(-1)
    
    def evaluate_actions(self, states, actions):
        """è©•ä¼°å‹•ä½œ"""
        policy, value = self.forward(states)
        probs = torch.softmax(policy, dim=-1)
        dist = Categorical(probs)
        log_probs = dist.log_prob(actions)
        entropy = dist.entropy()
        return log_probs, value.squeeze(-1), entropy

# ========== çå‹µå‡½æ•¸ ===========

# ç°¡å–®è·é›¢çå‹µé…ç½® (å‚™ç”¨)
DISTANCE_REWARD_CONFIG = {
    'forward_reward': 1.0,
    'backward_penalty': -0.5,
    'death_penalty': -50,
    'flag_reward': 1000,
    'time_penalty': -0.01,
}

def distance_only_reward(info, prev_info):
    """ç´”è·é›¢çå‹µ (å‚™ç”¨)"""
    reward = 0
    x_diff = info['x_pos'] - prev_info['x_pos']
    
    if x_diff > 0:
        reward += x_diff * DISTANCE_REWARD_CONFIG['forward_reward']
    elif x_diff < 0:
        reward += x_diff * abs(DISTANCE_REWARD_CONFIG['backward_penalty'])
    
    reward += DISTANCE_REWARD_CONFIG['time_penalty']
    
    if info.get('flag_get', False):
        reward += DISTANCE_REWARD_CONFIG['flag_reward']
    
    if info['life'] < prev_info['life']:
        reward += DISTANCE_REWARD_CONFIG['death_penalty']
    
    return reward

def get_reward(env, info, base_reward, prev_info):
    """æ ¹æ“šé…ç½®é¸æ“‡çå‹µå‡½æ•¸"""
    if USE_SMART_REWARD:
        return calculate_smart_reward(env, info, base_reward, prev_info)
    else:
        return distance_only_reward(info, prev_info)

# ========== ç’°å¢ƒåŒ…è£ ===========
def make_env():
    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')
    if isinstance(env, gym.wrappers.TimeLimit):
        env = env.env
    env = StepAPICompatibility(env, output_truncation_bool=False)
    env = JoypadSpace(env, SIMPLE_MOVEMENT)
    return env

# ========== Rollout Buffer ===========
class RolloutBuffer:
    def __init__(self):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
        
    def add(self, state, action, log_prob, reward, value, done):
        self.states.append(state)
        self.actions.append(action)
        self.log_probs.append(log_prob)
        self.rewards.append(reward)
        self.values.append(value)
        self.dones.append(done)
        
    def clear(self):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
        
    def compute_returns_and_advantages(self, last_value, gamma, gae_lambda):
        """è¨ˆç®— GAE å„ªå‹¢å’Œå›å ±"""
        returns = []
        advantages = []
        gae = 0
        
        values = self.values + [last_value]
        
        for step in reversed(range(len(self.rewards))):
            delta = self.rewards[step] + gamma * values[step + 1] * (1 - self.dones[step]) - values[step]
            gae = delta + gamma * gae_lambda * (1 - self.dones[step]) * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[step])
            
        return returns, advantages

# ========== PPO è¨“ç·´ ===========
def train_ppo():
    reward_type = "Smart Reward" if USE_SMART_REWARD else "Distance-Only"
    print(f"ğŸ® Starting PPO training with {reward_type}...")
    print(f"ğŸ“ Goal: Go as far as possible!")
    print(f"ğŸ”§ Config: {NUM_ENVS} envs, {TOTAL_TIMESTEPS} steps, lr={LEARNING_RATE}")
    
    # ç’°å¢ƒ
    env = make_env()
    obs_shape = (1, 84, 84)
    n_actions = len(SIMPLE_MOVEMENT)
    
    # ç¶²è·¯
    model = ActorCritic(obs_shape, n_actions).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # çµ±è¨ˆ
    best_distance = 0
    best_avg_distance = 0
    episode_count = 0
    total_steps = 0
    episode_rewards = []
    episode_distances = []
    
    # åˆå§‹åŒ–
    state = env.reset()
    state = preprocess_frame(state)
    state = np.expand_dims(state, axis=0)
    prev_info = {"x_pos": 40, "y_pos": 0, "score": 0, "coins": 0, 
                 "time": 400, "flag_get": False, "life": 3}
    
    buffer = RolloutBuffer()
    
    pbar = tqdm(total=TOTAL_TIMESTEPS, desc="PPO Training")
    
    current_episode_reward = 0
    current_max_x = 0
    
    while total_steps < TOTAL_TIMESTEPS:
        # æ”¶é›† rollout
        for _ in range(ROLLOUT_STEPS):
            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            
            with torch.no_grad():
                action, log_prob, value = model.get_action(state_tensor)
            
            action_np = action.cpu().numpy()[0]
            next_state, base_reward, done, info = env.step(action_np)
            
            # ä½¿ç”¨æ™ºæ…§çå‹µæˆ–ç´”è·é›¢çå‹µ
            reward = get_reward(env, info, base_reward, prev_info)
            
            # è™•ç†ç‹€æ…‹
            next_state_processed = preprocess_frame(next_state)
            next_state_processed = np.expand_dims(next_state_processed, axis=0)
            
            # å­˜å…¥ buffer
            buffer.add(
                state,
                action.cpu().numpy()[0],
                log_prob.cpu().numpy()[0],
                reward,
                value.cpu().numpy()[0],
                done
            )
            
            state = next_state_processed
            prev_info = info
            current_episode_reward += reward
            current_max_x = max(current_max_x, info['x_pos'])
            total_steps += 1
            pbar.update(1)
            
            if done:
                episode_count += 1
                episode_rewards.append(current_episode_reward)
                episode_distances.append(current_max_x)
                
                os.makedirs("ckpt_ppo", exist_ok=True)
                
                # 1. ä¿å­˜æœ€ä½³å–®æ¬¡è·é›¢æ¨¡å‹
                if current_max_x > best_distance:
                    best_distance = current_max_x
                    model_path = f"ckpt_ppo/ppo_best_single_distance_{int(best_distance)}_ep_{episode_count}.pth"
                    torch.save(model.state_dict(), model_path)
                    print(f"\nğŸƒ New best single distance: {best_distance}")
                
                # 2. ä¿å­˜æœ€ä½³å¹³å‡è·é›¢æ¨¡å‹ (æœ€è¿‘ 100 å€‹ episode)
                if len(episode_distances) >= 10:  # è‡³å°‘æœ‰ 10 å€‹ episode æ‰è¨ˆç®—å¹³å‡
                    current_avg_dist = np.mean(episode_distances[-100:])
                    if current_avg_dist > best_avg_distance:
                        best_avg_distance = current_avg_dist
                        model_path = f"ckpt_ppo/ppo_best_avg_distance_{int(best_avg_distance)}_ep_{episode_count}.pth"
                        torch.save(model.state_dict(), model_path)
                        print(f"\nğŸ“Š New best average distance: {best_avg_distance:.0f}")
                
                # 3. æ¯ 500 å€‹ episode ä¿å­˜ checkpoint
                if episode_count > 0 and episode_count % 500 == 0:
                    model_path = f"ckpt_ppo/ppo_checkpoint_ep_{episode_count}.pth"
                    torch.save(model.state_dict(), model_path)
                    print(f"\nğŸ“ Checkpoint saved: ep_{episode_count}")
                
                # æ›´æ–°é€²åº¦æ¢
                avg_dist = np.mean(episode_distances[-100:]) if episode_distances else 0
                pbar.set_postfix({
                    'ep': episode_count,
                    'avg_dist': f'{avg_dist:.0f}',
                    'best_avg': f'{best_avg_distance:.0f}',
                    'best': f'{best_distance:.0f}'
                })
                
                # é‡ç½®
                state = env.reset()
                state = preprocess_frame(state)
                state = np.expand_dims(state, axis=0)
                prev_info = {"x_pos": 40, "y_pos": 0, "score": 0, "coins": 0, 
                             "time": 400, "flag_get": False, "life": 3}
                current_episode_reward = 0
                current_max_x = 0
                
                # é‡ç½®çå‹µè¿½è¹¤è®Šæ•¸
                if USE_SMART_REWARD:
                    reset_max_x()
                    reset_enemy_tracking()
                    reset_hole_tracking()
        
        # PPO æ›´æ–°
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            _, last_value = model(state_tensor)
            last_value = last_value.cpu().numpy()[0][0]
        
        returns, advantages = buffer.compute_returns_and_advantages(last_value, GAMMA, GAE_LAMBDA)
        
        # è½‰æ›ç‚º tensor
        states = torch.tensor(np.array(buffer.states), dtype=torch.float32, device=device)
        actions = torch.tensor(np.array(buffer.actions), dtype=torch.long, device=device)
        old_log_probs = torch.tensor(np.array(buffer.log_probs), dtype=torch.float32, device=device)
        returns = torch.tensor(returns, dtype=torch.float32, device=device)
        advantages = torch.tensor(advantages, dtype=torch.float32, device=device)
        
        # æ­£è¦åŒ–å„ªå‹¢
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO epochs
        for _ in range(PPO_EPOCHS):
            # Mini-batch
            indices = np.random.permutation(len(buffer.states))
            
            for start in range(0, len(buffer.states), BATCH_SIZE):
                end = start + BATCH_SIZE
                batch_indices = indices[start:end]
                
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]
                
                # è©•ä¼°
                new_log_probs, values, entropy = model.evaluate_actions(batch_states, batch_actions)
                
                # è¨ˆç®—æ¯”ç‡
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                
                # Clipped ç›®æ¨™
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * batch_advantages
                
                # æå¤±
                actor_loss = -torch.min(surr1, surr2).mean()
                critic_loss = nn.MSELoss()(values, batch_returns)
                entropy_loss = -entropy.mean()
                
                loss = actor_loss + VALUE_COEF * critic_loss + ENTROPY_COEF * entropy_loss
                
                # æ›´æ–°
                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
                optimizer.step()
        
        buffer.clear()
    
    pbar.close()
    env.close()
    
    print(f"\nâœ… Training complete!")
    print(f"ğŸ“Š Best distance: {best_distance}")
    print(f"ğŸ“ˆ Average last 100 distances: {np.mean(episode_distances[-100:]):.1f}")

if __name__ == "__main__":
    train_ppo()
