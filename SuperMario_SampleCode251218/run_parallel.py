"""
å¹³è¡Œè¨“ç·´ Super Mario Bros DQN
ä½¿ç”¨å¤šå€‹ç’°å¢ƒåŒæ™‚æ”¶é›†ç¶“é©—ï¼ŒåŠ é€Ÿè¨“ç·´

16GB VRAM å»ºè­°ä½¿ç”¨ 4-8 å€‹å¹³è¡Œç’°å¢ƒ
"""
import os
import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm
from multiprocessing import Process, Queue, Manager
import time

import gym
import gym_super_mario_bros
from gym.wrappers import StepAPICompatibility
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT

from utils import preprocess_frame
from reward import calculate_smart_reward, reset_max_x, reset_enemy_tracking, reset_hole_tracking
from model import CustomCNN
from DQN import DQN, ReplayMemory

# ========== å¹³è¡Œè¨“ç·´é…ç½® ===========
NUM_ENVS = 8                    # å¹³è¡Œç’°å¢ƒæ•¸é‡ (æ ¹æ“š CPU å’Œ VRAM èª¿æ•´)
LR = 0.0001                     # å­¸ç¿’ç‡
BATCH_SIZE = 128                 # æ‰¹æ¬¡å¤§å° (å¤šç’°å¢ƒå¯ä»¥ç”¨æ›´å¤§çš„æ‰¹æ¬¡)
GAMMA = 0.99                    
MEMORY_SIZE = 100000         # æ›´å¤§çš„è¨˜æ†¶é«”
EPSILON_START = 1.0             # å¾é«˜æ¢ç´¢é–‹å§‹
EPSILON_END = 0.1               # æœ€çµ‚æ¢ç´¢ç‡
EPSILON_DECAY = 0.9995          # æ¢ç´¢ç‡è¡°æ¸›
TARGET_UPDATE = 100             
TOTAL_TIMESTEPS = 2000          # ç¸½è¨“ç·´å›åˆ
VISUALIZE = False               # å¹³è¡Œè¨“ç·´æ™‚é—œé–‰æ¸²æŸ“
MAX_STAGNATION_STEPS = 300      
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ========== å»ºç«‹ç’°å¢ƒå‡½æ•¸ ===========
def make_env():
    """å»ºç«‹ä¸€å€‹ Mario ç’°å¢ƒ"""
    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')
    if isinstance(env, gym.wrappers.TimeLimit):
        env = env.env
    env = StepAPICompatibility(env, output_truncation_bool=False)
    env = JoypadSpace(env, SIMPLE_MOVEMENT)
    return env

# ========== ç’°å¢ƒå·¥ä½œé€²ç¨‹ ===========
class EnvWorker:
    """ç®¡ç†å¤šå€‹å¹³è¡Œç’°å¢ƒ"""
    def __init__(self, num_envs):
        self.num_envs = num_envs
        self.envs = [make_env() for _ in range(num_envs)]
        self.states = [None] * num_envs
        self.prev_infos = [None] * num_envs
        self.dones = [True] * num_envs
        
    def reset_all(self):
        """é‡ç½®æ‰€æœ‰ç’°å¢ƒ"""
        for i in range(self.num_envs):
            state = self.envs[i].reset()
            state = preprocess_frame(state)
            state = np.expand_dims(state, axis=0)
            self.states[i] = state
            self.prev_infos[i] = {
                "x_pos": 0, "y_pos": 0, "score": 0,
                "coins": 0, "time": 400, "flag_get": False, "life": 3
            }
            self.dones[i] = False
        return self.states.copy()
    
    def reset_env(self, idx):
        """é‡ç½®å–®å€‹ç’°å¢ƒ"""
        state = self.envs[idx].reset()
        state = preprocess_frame(state)
        state = np.expand_dims(state, axis=0)
        self.states[idx] = state
        self.prev_infos[idx] = {
            "x_pos": 0, "y_pos": 0, "score": 0,
            "coins": 0, "time": 400, "flag_get": False, "life": 3
        }
        self.dones[idx] = False
        return state
    
    def step(self, actions):
        """åœ¨æ‰€æœ‰ç’°å¢ƒä¸­åŸ·è¡Œå‹•ä½œ"""
        results = []
        for i, action in enumerate(actions):
            if self.dones[i]:
                # å¦‚æœç’°å¢ƒå·²çµæŸï¼Œé‡ç½®
                self.reset_env(i)
                results.append((self.states[i], 0, False, self.prev_infos[i], 0))
                continue
            
            next_state, reward, done, info = self.envs[i].step(action)
            
            # é è™•ç†
            next_state_processed = preprocess_frame(next_state)
            next_state_processed = np.expand_dims(next_state_processed, axis=0)
            
            # è¨ˆç®—æ™ºæ…§çå‹µ
            custom_reward = calculate_smart_reward(
                self.envs[i], info, reward, self.prev_infos[i]
            )
            
            # æ›´æ–°ç‹€æ…‹
            self.states[i] = next_state_processed
            self.prev_infos[i] = info
            self.dones[i] = done
            
            results.append((
                next_state_processed,
                reward,
                done,
                info,
                custom_reward
            ))
        
        return results
    
    def close(self):
        """é—œé–‰æ‰€æœ‰ç’°å¢ƒ"""
        for env in self.envs:
            env.close()

# ========== ä¸»è¨“ç·´å¾ªç’° ===========
def train():
    print(f"ğŸ® Starting parallel training with {NUM_ENVS} environments...")
    
    # åˆå§‹åŒ–
    obs_shape = (1, 84, 84)
    n_actions = len(SIMPLE_MOVEMENT)
    
    # DQN
    dqn = DQN(
        model=CustomCNN,
        state_dim=obs_shape,
        action_dim=n_actions,
        learning_rate=LR,
        gamma=GAMMA,
        epsilon=EPSILON_START,
        target_update=TARGET_UPDATE,
        device=device
    )
    
    # ç¶“é©—å›æ”¾
    memory = ReplayMemory(MEMORY_SIZE)
    
    # ç’°å¢ƒç®¡ç†å™¨
    env_worker = EnvWorker(NUM_ENVS)
    
    # çµ±è¨ˆ
    total_steps = 0
    episode_rewards = []
    episode_max_x_list = []         # è¿½è¹¤æ¯å€‹ episode çš„æœ€é è·é›¢
    best_reward = -float('inf')
    best_max_x = 0                  # è¿½è¹¤æ­·å²æœ€é å–®æ¬¡è·é›¢
    best_avg_distance = 0           # ğŸ¯ è¿½è¹¤æœ€ä½³å¹³å‡è·é›¢ (æœ€é‡è¦ï¼)
    best_avg_reward = -float('inf') # è¿½è¹¤æœ€ä½³å¹³å‡çå‹µ
    epsilon = EPSILON_START
    
    # åˆå§‹åŒ–ç’°å¢ƒ
    states = env_worker.reset_all()
    
    # è¨“ç·´
    pbar = tqdm(total=TOTAL_TIMESTEPS, desc="Training")
    episode_count = 0
    current_rewards = [0] * NUM_ENVS
    current_max_x = [0] * NUM_ENVS  # æ¯å€‹ç’°å¢ƒçš„ç•¶å‰æœ€é è·é›¢
    
    while episode_count < TOTAL_TIMESTEPS:
        # ç‚ºæ¯å€‹ç’°å¢ƒé¸æ“‡å‹•ä½œ
        actions = []
        for i, state in enumerate(states):
            if np.random.rand() < epsilon:
                actions.append(np.random.randint(n_actions))
            else:
                with torch.no_grad():
                    state_tensor = torch.tensor([state], dtype=torch.float32, device=device)
                    q_values = dqn.q_net(state_tensor)
                    actions.append(q_values.argmax(dim=1).item())
        
        # åœ¨æ‰€æœ‰ç’°å¢ƒä¸­åŸ·è¡Œå‹•ä½œ
        results = env_worker.step(actions)
        
        # è™•ç†çµæœ
        for i, (next_state, reward, done, info, custom_reward) in enumerate(results):
            # å­˜å…¥è¨˜æ†¶é«”
            memory.push(states[i], actions[i], custom_reward, next_state, done)
            
            # æ›´æ–°ç‹€æ…‹
            states[i] = next_state
            current_rewards[i] += reward
            
            # è¿½è¹¤é€™å€‹ç’°å¢ƒçš„æœ€é è·é›¢
            current_max_x[i] = max(current_max_x[i], info.get('x_pos', 0))
            total_steps += 1
            
            # å¦‚æœå›åˆçµæŸ
            if done:
                ep_reward = current_rewards[i]
                ep_max_x = current_max_x[i]
                
                episode_rewards.append(ep_reward)
                episode_max_x_list.append(ep_max_x)
                
                # ========== ä¿å­˜æœ€ä½³æ¨¡å‹ (å°ˆæ³¨æ–¼å¹³å‡è¡¨ç¾) ==========
                os.makedirs("ckpt_parallel_average", exist_ok=True)
                
                # è¨ˆç®—ç§»å‹•å¹³å‡ (æœ€è¿‘ 50 å€‹ episode)
                if len(episode_max_x_list) >= 50:
                    current_avg_dist = np.mean(episode_max_x_list[-50:])
                    current_avg_reward = np.mean(episode_rewards[-50:])
                    
                    # 1. ğŸ¯ ä¸»è¦ï¼šåŸºæ–¼ã€Œå¹³å‡è·é›¢ã€ä¿å­˜ (æœ€é‡è¦ï¼ç©©å®šæ€§æŒ‡æ¨™)
                    if current_avg_dist > best_avg_distance:
                        best_avg_distance = current_avg_dist
                        # åˆªé™¤èˆŠçš„å¹³å‡è·é›¢æ¨¡å‹
                        for old_model in os.listdir("ckpt_parallel_average"):
                            if old_model.startswith("best_avg_distance_"):
                                try:
                                    os.remove(os.path.join("ckpt_parallel_average", old_model))
                                except:
                                    pass
                        model_path = f"ckpt_parallel_average/best_avg_distance_{int(best_avg_distance)}_ep_{episode_count}.pth"
                        torch.save(dqn.q_net.state_dict(), model_path)
                        print(f"\nğŸ“Š New best AVG distance: {best_avg_distance:.0f} (last 50 eps)")
                    
                    # 2. åŸºæ–¼ã€Œå¹³å‡çå‹µã€ä¿å­˜
                    if current_avg_reward > best_avg_reward:
                        best_avg_reward = current_avg_reward
                        # åˆªé™¤èˆŠçš„å¹³å‡çå‹µæ¨¡å‹
                        for old_model in os.listdir("ckpt_parallel_average"):
                            if old_model.startswith("best_avg_reward_"):
                                try:
                                    os.remove(os.path.join("ckpt_parallel_average", old_model))
                                except:
                                    pass
                        model_path = f"ckpt_parallel_average/best_avg_reward_{int(best_avg_reward)}_ep_{episode_count}.pth"
                        torch.save(dqn.q_net.state_dict(), model_path)
                        print(f"\nğŸ’° New best AVG reward: {best_avg_reward:.0f} (last 50 eps)")
                
                # 3. è¨˜éŒ„å–®æ¬¡æœ€ä½³ (åƒ…ä¾›åƒè€ƒï¼Œä¸ä½œç‚ºä¸»è¦æŒ‡æ¨™)
                if ep_max_x > best_max_x:
                    best_max_x = ep_max_x
                    # ä¸å†ä¿å­˜å–®æ¬¡æœ€ä½³æ¨¡å‹ï¼Œåªè¨˜éŒ„
                
                # 4. æ¯ 200 episode ä¿å­˜ä¸€æ¬¡ checkpoint
                if episode_count > 0 and episode_count % 200 == 0:
                    model_path = f"ckpt_parallel_average/checkpoint_ep_{episode_count}.pth"
                    torch.save(dqn.q_net.state_dict(), model_path)
                    print(f"\nğŸ“ Checkpoint saved: ep_{episode_count}")
                
                # é‡ç½®
                current_rewards[i] = 0
                current_max_x[i] = 0
                reset_max_x()
                reset_enemy_tracking()
                reset_hole_tracking()
                episode_count += 1
                pbar.update(1)
                
                # æ›´æ–°é€²åº¦æ¢ (å°ˆæ³¨æ–¼å¹³å‡å€¼)
                avg_reward = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards) if episode_rewards else 0
                avg_max_x = np.mean(episode_max_x_list[-50:]) if len(episode_max_x_list) >= 50 else np.mean(episode_max_x_list) if episode_max_x_list else 0
                pbar.set_postfix({
                    'avg_dist': f'{avg_max_x:.0f}',
                    'best_avg': f'{best_avg_distance:.0f}',
                    'best_single': f'{best_max_x:.0f}',
                    'Îµ': f'{epsilon:.3f}'
                })
        
        # è¨“ç·´
        if len(memory) >= BATCH_SIZE:
            batch = memory.sample(BATCH_SIZE)
            state_dict = {
                'states': batch[0],
                'actions': batch[1],
                'rewards': batch[2],
                'next_states': batch[3],
                'dones': batch[4],
            }
            dqn.train_per_step(state_dict)
        
        # æ›´æ–° epsilon
        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)
        dqn.epsilon = epsilon
    
    pbar.close()
    env_worker.close()
    
    print(f"\nâœ… Training complete!")
    print(f"ğŸ“Š Best reward: {best_reward}")
    print(f"ğŸ“ˆ Average last 100 rewards: {np.mean(episode_rewards[-100:]):.1f}")

if __name__ == "__main__":
    train()
