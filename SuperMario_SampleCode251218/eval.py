import numpy as np
import torch
from tqdm import tqdm
import os
import time  # ç”¨æ–¼æ§åˆ¶éŠæˆ²é€Ÿåº¦

import gym_super_mario_bros
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT

from utils import preprocess_frame
from model import CustomCNN
from DQN import DQN

# ========== Config ===========
# MODEL_PATH = os.path.join("ckpt_test","step_18_reward_536_custom_586.pth")        # æ¨¡å‹æ¬Šé‡æª”æ¡ˆçš„å­˜æ”¾è·¯å¾‘
# MODEL_PATH = "ckpt_parallel/best_reward_2878_ep_597.pth"
# MODEL_PATH = "ckpt_parallel/best_reward_2953_ep_105.pth"
MODEL_PATH = "ckpt_ppo/ppo_best_avg_distance_521_ep_2649.pth"  # ğŸ† é€šé—œæ¨¡å‹ï¼

#env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')                     # å»ºç«‹ã€Šè¶…ç´šç‘ªåˆ©æ­å…„å¼Ÿã€‹çš„éŠæˆ²ç’°å¢ƒ(ç¬¬1å€‹ä¸–ç•Œçš„ç¬¬1é—œ)

# SIMPLE_MOVEMENTå¯è‡ªè¡Œå®šç¾© ä»¥ä¸‹ç‚ºè‡ªè¨‚ç¯„ä¾‹:
# SIMPLE_MOVEMENT = [
#    # ["NOOP"],       # Do nothing.
#     ["right"],      # Move right.
#     ["right", "A"], # Move right and jump.
#     ["right", "B"], # Move right and run.
#     ["right", "A", "B"], # Move right, run, and jump.
#    # ["A"],          # Jump straight up.
#     ["left"],       # Move left.
#     ["left", "A"], # Move right and jump.
#     ["left", "B"], # Move right and run.
#     ["left", "A", "B"], # Move right, run, and jump.
# ]

#env = JoypadSpace(env, SIMPLE_MOVEMENT) 

import gym
from gym.wrappers import StepAPICompatibility

# 1) makeï¼ˆé€™è£¡å¯èƒ½æœƒè‡ªå‹•åŒ… TimeLimitï¼‰
env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')

# 2) ğŸ”‘ æ‹†æ‰ TimeLimitï¼ˆä¸æ‹†ä¸€å®šç‚¸ expected 5 got 4ï¼‰
if isinstance(env, gym.wrappers.TimeLimit):
    env = env.env

# 3) å›ºå®šæˆèˆŠ step APIï¼ˆå› 4-tupleï¼‰
env = StepAPICompatibility(env, output_truncation_bool=False)

# 4) å†åŒ… JoypadSpace
env = JoypadSpace(env, SIMPLE_MOVEMENT)

print("Final env:", env)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")       # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPUï¼Œå¦å‰‡ä½¿ç”¨ CPU ä½œç‚ºé‹ç®—è¨­å‚™
OBS_SHAPE = (1, 84, 84)                                                     # éŠæˆ²ç•«é¢è½‰æ›ç‚º (1, 84, 84) çš„ç°éšåœ–åƒ
N_ACTIONS = len(SIMPLE_MOVEMENT) 

VISUALIZE = True                                                            # æ˜¯å¦åœ¨æ¯å›åˆä¸­é¡¯ç¤ºéŠæˆ²ç•«é¢
FRAME_DELAY = 0.02                                                          # æ¯å¹€å»¶é²ç§’æ•¸ (0.02 = 50 FPS, 0.05 = 20 FPS, 0.1 = 10 FPS)
TOTAL_EPISODES = 10                                                         # æ¸¬è©¦å›åˆçš„ç¸½æ•¸
TEST_EPSILON = 0.05                                                         # æ¸¬è©¦æ™‚çš„å°æ¢ç´¢ç‡ï¼Œé¿å…å¡ä½
USE_SOFTMAX_SAMPLING = True                                                 # æ˜¯å¦ä½¿ç”¨ softmax æŠ½æ¨£ï¼ˆèˆ‡è¨“ç·´ä¸€è‡´ï¼‰

# ========== Initialize DQN =========== 
dqn = DQN( 
    model=CustomCNN, 
    state_dim=OBS_SHAPE,
    action_dim=N_ACTIONS,
    learning_rate=0.0001,  
    gamma=0.99,          
    epsilon=0.0,                   # è¨­ç‚º 0.0 è¡¨ç¤ºå®Œå…¨åˆ©ç”¨ç•¶ä¸‹çš„ç­–ç•¥
    target_update=1000,            # target [Q-net] æ›´æ–°çš„é »ç‡
    device=device
)

# ========== è¼‰å…¥æ¨¡å‹æ¬Šé‡ =========== 
if os.path.exists(MODEL_PATH):
    try:                                                                  # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼š
        model_weights = torch.load(MODEL_PATH, map_location=device)       #  è‹¥å­˜åœ¨ï¼Œå˜—è©¦è¼‰å…¥æ¨¡å‹æ¬Šé‡
        dqn.q_net.load_state_dict(model_weights)                          #    è¼‰å…¥æˆåŠŸï¼Œæ‡‰ç”¨åˆ°æ¨¡å‹
        dqn.q_net.eval()                                                  #    è¼‰å…¥å¤±æ•—ï¼Œè¼¸å‡ºå…·é«”çš„éŒ¯èª¤è³‡è¨Š(éŒ¯èª¤è³‡è¨Šå­˜åœ¨eä¸­)
        print(f"Model loaded successfully from {MODEL_PATH}")             #  è‹¥ä¸å­˜åœ¨ï¼Œå‰‡FileNotFoundError
    except Exception as e:
        print(f"Failed to load model weights: {e}")
        raise
else:
    raise FileNotFoundError(f"Model file not found: {MODEL_PATH}")

# ========== Evaluation Loop ===========
for episode in range(1, TOTAL_EPISODES + 1):
    state = env.reset()                                                   # é‡ç½®ç’°å¢ƒåˆ°åˆå§‹ç‹€æ…‹ï¼Œä¸¦ç²å–ç’°å¢ƒçš„ state åˆå§‹å€¼
    state = preprocess_frame(state)
    state = np.expand_dims(state, axis=0)                                 # æ–°å¢ channel dimension ( [H, W] to [1, H, W] )
    state = np.expand_dims(state, axis=0)                                 # æ–°å¢ batch dimension ( [1, H, W] to [1, 1, H, W] )
                                                                          # ç¬¦åˆ CNN è¼¸å…¥è¦æ±‚ï¼š[batch, channels, height, width]
    done = False
    total_reward = 0
    max_x_pos = 0                                                             # è¿½è¹¤æœ€é è·é›¢

    while not done:
        # å°æ©Ÿç‡æ¢ç´¢ï¼Œé¿å…å¡ä½
        if np.random.rand() < TEST_EPSILON:
            action = np.random.randint(N_ACTIONS)
        else:
            # Take action using the trained policy
            state_tensor = torch.tensor(state, dtype=torch.float32, device=device)
            with torch.no_grad():
                q_values = dqn.q_net(state_tensor)
                
                if USE_SOFTMAX_SAMPLING:
                    # ä½¿ç”¨ softmax æŠ½æ¨£ï¼ˆèˆ‡è¨“ç·´ä¸€è‡´ï¼‰
                    action_probs = torch.softmax(q_values, dim=1)
                    action_dist = torch.distributions.Categorical(action_probs)
                    action = action_dist.sample().item()
                else:
                    # ä½¿ç”¨ argmaxï¼ˆç´” greedyï¼‰
                    action = q_values.argmax(dim=1).item()
        
        next_state, reward, done, info = env.step(action)

        # Preprocess next state
        next_state = preprocess_frame(next_state)
        next_state = np.expand_dims(next_state, axis=0)                           # æ–°å¢ channel dimension
        next_state = np.expand_dims(next_state, axis=0)                           # æ–°å¢ batch dimension

        # Accumulate rewards
        total_reward += reward
        max_x_pos = max(max_x_pos, info.get('x_pos', 0))                          # è¿½è¹¤æœ€é è·é›¢
        state = next_state

        if VISUALIZE:                                                             # å¦‚æœ VISUALIZE=Trueï¼Œå‰‡ç”¨ env.render() é¡¯ç¤ºç’°å¢ƒç•¶ä¸‹çš„ state
            env.render()
            time.sleep(FRAME_DELAY)                                               # å»¶é²è®“ç•«é¢æ…¢ä¸€é»

    print(f"Episode {episode}/{TOTAL_EPISODES} - Reward: {total_reward} - Max X: {max_x_pos}")  # é¡¯ç¤ºçå‹µå’Œæœ€é è·é›¢

env.close()
