# 🎬 Super Mario RL 影片錄製腳本

> 影片需求：說明修改程式碼的部分及原因，訓練及測試各錄製 10 回合

---

## 📹 影片結構（建議總長 5-8 分鐘）

| 段落 | 時長 | 內容 |
|------|------|------|
| 開場 | 30秒 | 自我介紹、專案簡介 |
| 程式碼說明 | 2-3分鐘 | 展示 reward.py 修改內容 |
| 訓練錄影 | 2分鐘 | 錄製 10 回合訓練過程 |
| 測試錄影 | 1分鐘 | 錄製 10 回合測試過程 |
| 結論 | 30秒 | 總結觀察與心得 |

---

## 🎤 第一部分：開場（30 秒）

```
大家好，我是 [你的名字]。
這是 NCKU CVDL 2025 的 Super Mario 強化學習作業。

我的目標是設計獎勵函數，讓 AI 學會「快速通關」。
在這個影片中，我會說明我修改了哪些程式碼，
以及為什麼這樣設計，最後展示訓練和測試的結果。
```

---

## 🎤 第二部分：程式碼修改說明（2-3 分鐘）

### 📌 操作：開啟 reward.py，邊講邊滾動檔案

---

### 2.1 總覽說明（30 秒）

```
首先看 reward.py，這是本次作業的核心。

我總共實作了 16 種獎勵函數：
- 原有的 9 種基本規則
- 新增的 7 種「快速通關」專用規則

讓我分別說明這些修改。
```

---

### 2.2 模式設定（15 秒）

**📌 滾動到第 14 行，顯示 EXTREME_MODE**

```
這裡是模式切換開關。
EXTREME_MODE = True 是極端模式，會放大所有獎勵訊號。
這讓 AI 學得更快，但也更容易過度反應。
```

---

### 2.3 獎勵配置表（45 秒）

**📌 滾動到第 23-48 行，顯示 NORMAL_CONFIG**

```
這是 Normal 模式的獎勵配置表，讓我說明重點：

核心獎勵：
- forward_reward = 30：前進是最重要的，每步給 30 分
- flag_reward = 1000：通關給 1000 分的大獎勵
- speed_bonus = 15：這是我新增的，獎勵快速跑動

核心懲罰：
- death_penalty = -300：死亡扣 300 分，非常嚴重
- backward_penalty = -25：後退扣分
- stagnation_penalty = -20：原地不動扣分

特別注意：
- fall_penalty = 0：下落不扣分，這很重要！
  因為跳躍一定會下落，如果扣分 AI 會不敢跳
```

**📌 滾動到第 56-81 行，顯示 EXTREME_CONFIG**

```
Extreme 模式是策略性放大：
- 核心獎勵放大 3-4 倍（forward: 30→100）
- 核心懲罰放大 2 倍（death: -300→-500）

這不是簡單的 x10，而是有策略地強化重要訊號。
```

---

### 2.4 新增的 7 個獎勵函數（1 分鐘）

**📌 滾動到各個新函數的位置**

```
接下來是我新增的 7 個快速通關獎勵函數。
```

#### 函數 10：speed_bonus_reward

**📌 滾動到約第 220 行**

```
第一個是 speed_bonus_reward。
當 Mario 快速移動（x_diff > 5）時給予獎勵。
這鼓勵 AI 按住 B 鍵跑動，而不是慢慢走。
```

#### 函數 11：momentum_bonus_reward

**📌 滾動到約第 230 行**

```
第二個是 momentum_bonus_reward。
當連續前進超過 5 步時給予額外獎勵。
這避免 AI 走走停停，維持前進動量。
```

#### 函數 12：new_distance_record_reward

**📌 滾動到約第 245 行**

```
第三個是 new_distance_record_reward。
當 AI 到達本回合的最遠位置時給予獎勵。
這鼓勵探索，激勵 AI 突破記錄。
```

#### 函數 13：obstacle_clear_reward

**📌 滾動到約第 260 行**

```
第四個是 obstacle_clear_reward。
當 AI 從高處落下且有明顯前進時給予獎勵。
這表示成功跳過了障礙物或敵人。
```

#### 函數 14：progress_efficiency_reward

**📌 滾動到約第 275 行**

```
第五個是 progress_efficiency_reward。
根據位置和剩餘時間計算效率。
如果進度落後於時間，會受到懲罰。
```

#### 函數 15：precise_jump_reward

**📌 滾動到約第 290 行**

```
第六個是 precise_jump_reward。
當跳躍很高且同時有前進時給予獎勵。
這鼓勵精準跳躍，而非隨意跳。
```

#### 函數 16：perfect_clear_bonus

**📌 滾動到約第 305 行**

```
最後是 perfect_clear_bonus。
通關時，根據剩餘時間給予額外獎勵。
時間剩越多，獎勵越高，鼓勵快速過關。
```

---

### 2.5 總獎勵計算（15 秒）

**📌 滾動到約第 315 行，顯示 final_reward 函數**

```
最後在 final_reward 函數中，
整合了所有 16 個獎勵函數的結果。
每一步的獎勵就是這些函數的總和。
```

---

## 🎤 第三部分：訓練錄影（2 分鐘）

### 📌 操作步驟

1. 開啟終端機
2. 確認 `VISUALIZE = True`（run.py 第 53 行）
3. 執行訓練指令

---

### 3.1 開始錄製前說明（15 秒）

```
現在開始錄製訓練過程，我會錄製 10 回合。
注意觀察畫面上方的 Epsilon 值，
它會從 1.0 逐漸下降，代表探索率降低。
```

### 3.2 執行訓練

**📌 在終端機執行：**

```bash
cd ~/Desktop/NCKU-CVDL-2025/SuperMario_SampleCode251218
python run.py
```

### 3.3 訓練過程講解（錄製 10 回合期間）

```
【回合 1-3】
現在 Epsilon 約等於 1，AI 是完全隨機行動。
可以看到它到處亂跑，經常撞到敵人死亡。
這是探索階段，AI 在嘗試各種可能。

【回合 4-6】
Epsilon 開始下降到約 0.99。
AI 還是很隨機，但偶爾會有一些向右的傾向。
這是因為前進有獎勵，AI 開始學到一點規律。

【回合 7-10】
隨著訓練繼續，AI 會慢慢學會：
1. 往右走會得到獎勵
2. 碰到敵人會死亡被扣分
3. 跳躍可以避開障礙

注意看 cumulative_reward 的變化，
數字越來越大代表 AI 學得越來越好。
```

### 3.4 訓練結束說明（15 秒）

```
10 回合錄製完成。
可以看到 AI 從完全隨機，到開始學會向右移動。
這就是強化學習的過程：透過獎勵引導行為。
```

---

## 🎤 第四部分：測試錄影（1 分鐘）

### 📌 操作步驟

1. 修改 eval.py 中的 MODEL_PATH
2. 執行測試指令

---

### 4.1 開始錄製前說明（15 秒）

```
現在錄製測試過程。
我會載入訓練好的最佳模型來測試。
這個模型是 Extreme 模式訓練到 step 1368，
獲得 106,766 的最高獎勵。
```

### 4.2 執行測試

**📌 先修改 eval.py 第 18 行：**

```python
MODEL_PATH = os.path.join("liang_test_extreme", "step_1368_reward_106766.pth")
```

**📌 在終端機執行：**

```bash
python eval.py
```

### 4.3 測試過程講解（錄製 10 回合期間）

```
【觀察重點 1：移動速度】
可以看到 AI 不是慢慢走，而是快速跑動。
這就是 speed_bonus 獎勵的效果。

【觀察重點 2：跳躍行為】
AI 會在適當的時機跳躍避開敵人。
這是因為我們給死亡很大的懲罰 (-500)。

【觀察重點 3：最遠距離】
注意看 x_pos 的數值。
我訓練的模型可以到達 x = 1072。
這已經越過了第一個敵人的位置。

【若 AI 死亡】
AI 還是會死亡，這是正常的。
2000 回合的訓練量不足以學會完美通關。
但相比隨機行動，AI 已經進步很多。
```

### 4.4 測試結束說明（15 秒）

```
10 回合測試完成。
最佳結果是 x_pos = 1072，總獎勵約 1051。
這證明我們的獎勵函數設計是有效的。
```

---

## 🎤 第五部分：結論（30 秒）

```
總結這次作業：

【程式碼修改】
我在 reward.py 中實作了 16 個獎勵函數，
其中 7 個是專門為快速通關新增的。

關鍵設計有三點：
1. 速度獎勵：讓 AI 學會跑而不是走
2. 動量獎勵：維持連續前進
3. 下落懲罰為零：讓 AI 敢於跳躍

【結果】
使用 Extreme 模式訓練後，
AI 的 x_pos 從約 200 提升到 1072，
成功越過了第一個敵人。

以上就是我的作業報告，謝謝觀看。
```

---

## 📝 錄製注意事項

### 錄製軟體建議
- **Linux**: OBS Studio, SimpleScreenRecorder
- **Windows**: OBS Studio, Bandicam

### 錄製設定
- 解析度：1920x1080 或 1280x720
- 幀率：30 FPS
- 音訊：確保麥克風有收音

### 錄製 Checklist

- [ ] 確認 `VISUALIZE = True`（訓練時）
- [ ] 確認麥克風有開啟
- [ ] 準備好要展示的程式碼位置
- [ ] 測試音量是否適中
- [ ] 確認螢幕不會顯示敏感資訊

---

## 🎯 快速指令參考

```bash
# 訓練（需開啟 VISUALIZE = True）
cd ~/Desktop/NCKU-CVDL-2025/SuperMario_SampleCode251218
python run.py

# 測試 Extreme 模式模型
# 先修改 eval.py: MODEL_PATH = os.path.join("liang_test_extreme", "step_1368_reward_106766.pth")
python eval.py

# 測試 Normal 模式模型
# 先修改 eval.py: MODEL_PATH = os.path.join("liang_test", "step_1417_reward_37291.pth")
python eval.py
```
