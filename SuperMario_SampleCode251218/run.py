import os
import numpy as np
import random
import torch
import torch.nn as nn
import cv2
import time
from tqdm import tqdm

import gym_super_mario_bros                                      #å°å…¥gym_super_mario_brosï¼Œé€™æ˜¯ä¸€å€‹åŸºæ–¼ Gym çš„æ¨¡çµ„ï¼Œç”¨æ–¼æ¨¡æ“¬ã€ŠSuper Mario Brosã€‹éŠæˆ²ç’°å¢ƒã€‚
from nes_py.wrappers import JoypadSpace                          #å¾nes_pyä¸­å°å…¥JoypadSpaceï¼Œç”¨æ–¼é™åˆ¶éŠæˆ²ä¸­å¯ç”¨çš„æŒ‰éˆ•å‹•ä½œï¼ˆä¾‹å¦‚åƒ…å…è¨±ã€Œç§»å‹•å³ã€æˆ–ã€Œè·³èºã€çš„å‹•ä½œé›†åˆï¼‰ã€‚
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT         #å¾ gym_super_mario_brosä¸­å°å…¥SIMPLE_MOVEMENTï¼Œé€™æ˜¯ä¸€å€‹é å®šç¾©çš„æŒ‰éˆ•å‹•ä½œé›†åˆï¼ˆå¦‚ã€Œå³ç§»ã€ã€ã€Œè·³èºã€ç­‰ï¼‰ï¼Œç”¨æ–¼æ§åˆ¶ Mario çš„è¡Œç‚ºã€‚
                                                                 #ç°¡åŒ–å‹•ä½œç©ºé–“ NES æ§åˆ¶å™¨æœ‰ 8 å€‹æŒ‰éµï¼ˆä¸Šä¸‹å·¦å³ã€Aã€Bã€Selectã€Startï¼‰ï¼Œå¯èƒ½çš„æŒ‰éµçµ„åˆæ•¸éå¸¸å¤§

from utils import preprocess_frame                               #ç”¨æ–¼å°éŠæˆ²çš„ç•«é¢é€²è¡Œé è™•ç†ï¼Œä¾‹å¦‚ç°éšåŒ–ã€èª¿æ•´å¤§å°ç­‰ï¼Œå°‡å…¶è½‰æ›ç‚ºé©åˆç¥ç¶“ç¶²è·¯è¼¸å…¥çš„æ ¼å¼
from reward import *  
from reward import EXTREME_MODE                                           #æ¨¡çµ„ä¸­å°å…¥æ‰€æœ‰å‡½å¼ï¼Œé€™äº›å‡½å¼ç”¨æ–¼è¨­è¨ˆå’Œè¨ˆç®—è‡ªå®šç¾©çå‹µï¼ˆä¾‹å¦‚æ ¹æ“š Mario çš„ç¡¬å¹£æ•¸é‡ã€æ°´å¹³ä½ç§»ç­‰ä¾†è¨ˆç®—çå‹µï¼‰ã€‚
from model import CustomCNN                                      #è‡ªå®šç¾©çš„å·ç©ç¥ç¶“ç¶²è·¯æ¨¡å‹ï¼Œç”¨æ–¼è™•ç†éŠæˆ²ç•«é¢ä¸¦ç”Ÿæˆå‹•ä½œæ±ºç­–
from DQN import DQN, ReplayMemory                                #ç”¨æ–¼åŸ·è¡Œå¼·åŒ–å­¸ç¿’çš„ä¸»è¦é‚è¼¯ DQNæ¨¡çµ„ä¸­å°å…¥å›æ”¾è¨˜æ†¶é«”ï¼Œç”¨æ–¼å­˜å„²å’ŒæŠ½å–éŠæˆ²çš„ç‹€æ…‹ã€å‹•ä½œã€çå‹µç­‰æ¨£æœ¬ï¼Œæå‡è¨“ç·´ç©©å®šæ€§ã€‚



# ========== config ===========
#env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')   #
#env = JoypadSpace(env, SIMPLE_MOVEMENT)
import gym
from gym.wrappers import StepAPICompatibility

# 1) makeï¼ˆé€™è£¡å¯èƒ½æœƒè‡ªå‹•åŒ… TimeLimitï¼‰
env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')

# 2) ğŸ”‘ æ‹†æ‰ TimeLimitï¼ˆä¸æ‹†ä¸€å®šç‚¸ expected 5 got 4ï¼‰
if isinstance(env, gym.wrappers.TimeLimit):
    env = env.env

# 3) å›ºå®šæˆèˆŠ step APIï¼ˆå› 4-tupleï¼‰
env = StepAPICompatibility(env, output_truncation_bool=False)

# 4) å†åŒ… JoypadSpace
env = JoypadSpace(env, SIMPLE_MOVEMENT)

print("Final env:", env)

#========= basic train config==============================================
LR = 0.005                    
BATCH_SIZE = 64                 # æ‰¹æ¬¡å¤§å°
GAMMA = 0.99                    # æé«˜ï¼æ›´é‡è¦–é•·æœŸçå‹µï¼ˆå­¸æœƒè·³éæ•µäººï¼‰
MEMORY_SIZE = 50000             # è¨˜æ†¶é«”å¤§å°
EPSILON_START = 1.0             # æ–°å¢ï¼šåˆå§‹æ¢ç´¢ç‡ 100%
EPSILON_END = 0.1               # é™ä½ï¼šæœ€çµ‚æ¢ç´¢ç‡ 10%
EPSILON_DECAY = 0.995           # æ–°å¢ï¼šæ¯å›åˆæ¢ç´¢ç‡è¡°æ¸›
TARGET_UPDATE = 100             # ç›®æ¨™ç¶²è·¯æ›´æ–°é »ç‡
TOTAL_TIMESTEPS = 10            # è¨“ç·´å›åˆæ•¸ï¼ˆéŒ„å½±è¨­ 10ï¼‰
VISUALIZE = True               # æ˜¯å¦æ¸²æŸ“éŠæˆ²ç•«é¢
MAX_STAGNATION_STEPS = 300      # åœæ»¯æ­¥æ•¸ä¸Šé™
device = torch.device("cuda")

# åŠ é€Ÿè¨“ç·´è¨­å®š
FRAME_SKIP = 2                  # è®“ Mario æœ‰æ›´å¤šåæ‡‰æ™‚é–“è·³éæ•µäºº
TRAIN_FREQUENCY = 4             # æ¯ N æ­¥è¨“ç·´ä¸€æ¬¡
RENDER_DELAY = 0.02             # æ¸²æŸ“å»¶é²ï¼ˆç§’ï¼‰ï¼Œè¨­ 0 = æœ€å¿«ï¼Œ0.02 = æ­£å¸¸é€Ÿåº¦ï¼Œ0.05 = æ…¢é€Ÿ

# å½±ç‰‡éŒ„è£½è¨­å®š
RECORD_VIDEO = True                                         # æ˜¯å¦éŒ„è£½å½±ç‰‡
VIDEO_FPS = 30                                              # å½±ç‰‡å¹€ç‡
VIDEO_DIR = "videos"                                        # å½±ç‰‡å„²å­˜ç›®éŒ„
os.makedirs(VIDEO_DIR, exist_ok=True)                       # å»ºç«‹ç›®éŒ„
VIDEO_OUTPUT_PATH = os.path.join(VIDEO_DIR, f"mario_train_{'extreme' if EXTREME_MODE else 'normal'}.mp4")





# ========================DQN Initialization==========================================
obs_shape = (1, 84, 84)                         #obs_shape = (1, 84, 84)
n_actions = len(SIMPLE_MOVEMENT)                #å®šç¾©å‹•ä½œç©ºé–“å¤§å°ï¼Œä½¿ç”¨SIMPLE_MOVEMENTä¸­çš„å‹•ä½œæ•¸é‡ï¼ˆä¾‹å¦‚å‘å³ç§»å‹•ã€è·³èºç­‰ï¼‰
model = CustomCNN                               #æŒ‡å®šæ¨¡å‹æ¶æ§‹ç‚ºCustomCNNç”¨æ–¼è™•ç†åœ–åƒä¸¦é æ¸¬å„å‹•ä½œçš„ Q å€¼
dqn = DQN(                                      #åˆå§‹åŒ– DQN agent
    model=model,
    state_dim=obs_shape,                        #ç‹€æ…‹ç©ºé–“å¤§å°
    action_dim=n_actions,                       #å‹•ä½œç©ºé–“å¤§å°
    learning_rate=LR,                           #å­¸ç¿’ç‡
    gamma=GAMMA,                                #æŠ˜æ‰£å› å­ï¼Œç”¨æ–¼è¨ˆç®—æœªä¾†çå‹µ
    epsilon=EPSILON_START,                      # ä½¿ç”¨åˆå§‹æ¢ç´¢ç‡ 1.0
    target_update=TARGET_UPDATE,                #ç›®æ¨™ç¶²è·¯æ›´æ–°é »ç‡
    device=device
)

# ========== è¼‰å…¥é è¨“ç·´æ¬Šé‡ ==========
LOAD_PRETRAINED = True  # è¨­ç‚º True è¼‰å…¥é è¨“ç·´æ¨¡å‹ï¼ŒFalse å¾é ­è¨“ç·´
PRETRAINED_MODEL_PATH = os.path.join("liang_test_extreme", "step_1368_reward_106766.pth")

if LOAD_PRETRAINED and os.path.exists(PRETRAINED_MODEL_PATH):
    dqn.q_net.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device))
    dqn.tgt_q_net.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device))
    print(f"âœ… Loaded pretrained model from: {PRETRAINED_MODEL_PATH}")
    
    # è¼‰å…¥é è¨“ç·´æ¨¡å‹æ™‚ï¼Œé™ä½æ¢ç´¢ç‡
    EPSILON_START = 0.3  # å¾ 30% æ¢ç´¢é–‹å§‹ï¼ˆå› ç‚ºå·²æœ‰ç¶“é©—ï¼‰
    current_epsilon = EPSILON_START
    dqn.epsilon = current_epsilon
    TOTAL_TIMESTEPS = 10
else:
    if LOAD_PRETRAINED:
        print(f"âš ï¸ Pretrained model not found: {PRETRAINED_MODEL_PATH}")
    print("ğŸ”„ Training from scratch")

memory = ReplayMemory(MEMORY_SIZE)              #å‰µå»ºç¶“é©—å›æ”¾è¨˜æ†¶é«”ï¼Œç”¨æ–¼å­˜å„²ç‹€æ…‹è½‰ç§»
step = 0                                        #è¨˜éŒ„ç¸½æ­¥æ•¸
best_reward = -float('inf')                     # å„²å­˜æœ€ä½³ç´¯ç©çå‹µ
cumulative_reward = 0                           # ç•¶å‰æ™‚é–“æ­¥çš„ç¸½ç´¯ç©çå‹µ
current_epsilon = EPSILON_START                 # è¿½è¹¤ç•¶å‰æ¢ç´¢ç‡

# ========== åˆå§‹åŒ–å½±ç‰‡éŒ„è£½ ===========
video_writer = None
total_video_frames = 0
if RECORD_VIDEO:
    # å–å¾—éŠæˆ²ç•«é¢å°ºå¯¸
    sample_frame = env.reset()
    height, width = sample_frame.shape[:2]
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, VIDEO_FPS, (width, height))
    print(f"ğŸ¬ éŒ„è£½è¨“ç·´å½±ç‰‡: {VIDEO_OUTPUT_PATH}")
    print(f"   è§£æåº¦: {width}x{height}, FPS: {VIDEO_FPS}")




#=======================è¨“ç·´é–‹å§‹============================
for timestep in tqdm(range(1, TOTAL_TIMESTEPS + 1), desc="Training Progress"):  #ä¸»è¨“ç·´è¿´åœˆï¼Œé€²è¡ŒTOTAL_TIMESTEPSæ¬¡è¿­ä»£
    state = env.reset()                                                         #é‡ç½®éŠæˆ²ç’°å¢ƒï¼Œç²å–åˆå§‹ç‹€æ…‹
    state = preprocess_frame(state)                                             #ä½¿ç”¨preprocess_frame å°‡ç•«é¢è™•ç†ç‚ºç°éšã€ç¸®æ”¾ç‚º84x84
    state = np.expand_dims(state, axis=0)                                       #æ–°å¢ä¸€å€‹ç¶­åº¦ï¼Œé©é…æ¨¡å‹è¼¸å…¥

    done = False                                                                #è¡¨ç¤ºç•¶å‰éŠæˆ²æ˜¯å¦çµæŸ
    prev_info = {                                                               #ç”¨æ–¼è¿½è¹¤éŠæˆ²ç‹€æ…‹ï¼ˆå¦‚æ°´å¹³ä½ç½®ã€å¾—åˆ†ã€ç¡¬å¹£æ•¸é‡ï¼‰
        "x_pos": 0,  # Starting horizontal position (int).
        "y_pos": 0,  # Starting vertical position (int).
        "score": 0,  # Initial score is 0 (int).
        "coins": 0,  # Initial number of collected coins is 0 (int).
        "time": 400,  # Initial time in most levels of Super Mario Bros is 400 (int).
        "flag_get": False,  # Player has not yet reached the end flag (bool).
        "life": 3  # Default initial number of lives is 3 (int).
    }

    cumulative_reward = 0 
    stagnation_time = 0                                                           #stagnation_timeè¨˜éŒ„éŠæˆ²è§’è‰²åœ¨æ°´å¹³æ–¹å‘çš„åœæ»¯æ™‚é–“
    #é–‹å§‹ä¸€å€‹å›åˆçš„éŠæˆ²å¾ªç’°
    while not done:
        action = dqn.take_action(state)                                           #è¼¸å…¥ç›®å‰ç‹€æ…‹ï¼Œäº¤çµ¦DQNå»åšä¸‹ä¸€æ­¥
        
        # âš¡ Frame Skip: é‡è¤‡åŸ·è¡ŒåŒä¸€å‹•ä½œ N æ¬¡ï¼Œç´¯ç©çå‹µ
        frame_reward = 0
        raw_frame = None  # å„²å­˜åŸå§‹ç•«é¢ç”¨æ–¼éŒ„å½±
        for _ in range(FRAME_SKIP):
            next_state, reward, done, info = env.step(action)
            raw_frame = next_state.copy()  # ä¿å­˜åŸå§‹ RGB ç•«é¢
            frame_reward += reward
            if done:
                break
        reward = frame_reward  # ä½¿ç”¨ç´¯ç©çš„çå‹µ
       
        # preprocess image state å°‡ä¸‹ä¸€ç‹€æ…‹é€²è¡Œé è™•ç†ä¸¦èª¿æ•´ç‚ºé©åˆæ¨¡å‹çš„å½¢ç‹€
        next_state = preprocess_frame(next_state)
        next_state = np.expand_dims(next_state, axis=0)

        cumulative_reward += final_reward(info, reward, prev_info)   #æ›´æ–°ç´¯ç©çå‹µ


        # ===========================Check for x_pos stagnation  å¦‚æœè§’è‰²çš„æ°´å¹³ä½ç½®æœªæ”¹è®Šè¶…éMAX_STAGNATION_STEPSå‰‡å¼·åˆ¶çµæŸæœ¬å±€éŠæˆ²
        if info["x_pos"] == prev_info["x_pos"]:
            stagnation_time += 1
            if stagnation_time >= MAX_STAGNATION_STEPS:
                print(f"Timestep {timestep} - Early stop triggered due to x_pos stagnation.")
                done = True
        else:
            stagnation_time = 0
        
        
        #===========================Store transition in memory å°‡ç‹€æ…‹è½‰ç§» (state, action, reward, next_state, done) å­˜å…¥è¨˜æ†¶é«”
        memory.push(state, action, cumulative_reward //1, next_state, done)      #ä½¿ç”¨è‡ªè¨‚ç¾©çå‹µ
        #memory.push(state, action, final_reward(info, reward, prev_info), next_state, done)                  #ä½¿ç”¨å…¶é è¨­å¥½çš„çå‹µ
        #æ›´æ–°ç•¶å‰ç‹€æ…‹
        state = next_state

        #==============================Train DQN ç•¶è¨˜æ†¶é«”ä¸­æ¨£æœ¬æ•¸é‡é”åˆ°æ‰¹æ¬¡å¤§å°æ™‚ï¼Œå¾è¨˜æ†¶é«”ä¸­éš¨æ©ŸæŠ½å–ä¸€æ‰¹æ¨£æœ¬é€²è¡Œç¶²è·¯æ›´æ–°
        # âš¡ æ¯ TRAIN_FREQUENCY æ­¥æ‰è¨“ç·´ä¸€æ¬¡ï¼Œæ¸›å°‘è¨“ç·´é–‹éŠ·
        if len(memory) >= BATCH_SIZE and step % TRAIN_FREQUENCY == 0:
            batch = memory.sample(BATCH_SIZE)

            state_dict = {                                       #å°‡é€™äº›æ•¸æ“šæ‰“åŒ…ç‚ºå­—å…¸æ ¼å¼ï¼Œæ–¹ä¾¿å‚³éçµ¦æ¨¡å‹é€²è¡Œè¨“ç·´
                'states': batch[0],
                'actions': batch[1],
                'rewards': batch[2],
                'next_states': batch[3],
                'dones': batch[4],
            }
            dqn.train_per_step(state_dict)                       #train_per_stepæ˜¯DQNä¸­çš„æ–¹æ³•ï¼Œç”¨æ–¼è¨ˆç®—æå¤±ä¸¦æ›´æ–°ç¥ç¶“ç¶²è·¯çš„æ¬Šé‡

        #================================æ›´æ–°ç‹€æ…‹è¨Šæ¯
        prev_info = info
        step += 1

        if VISUALIZE:                                   #æ¸²æŸ“ç•¶å‰éŠæˆ²ç•«é¢
            env.render()
            time.sleep(RENDER_DELAY)                    # å»¶é²æ§åˆ¶é€Ÿåº¦
        
        # éŒ„è£½å½±ç‰‡å¹€
        if RECORD_VIDEO and video_writer is not None and raw_frame is not None:
            frame_rgb = cv2.cvtColor(raw_frame, cv2.COLOR_RGB2BGR)
            video_writer.write(frame_rgb)
            total_video_frames += 1

    # âš¡ Epsilon Decay: æ¯å›åˆçµæŸå¾Œé™ä½æ¢ç´¢ç‡
    current_epsilon = max(EPSILON_END, current_epsilon * EPSILON_DECAY)
    dqn.epsilon = current_epsilon

    # Print cumulative reward for the current timestep
    print(f"Timestep {timestep} - Reward: {cumulative_reward:.0f} - Epsilon: {current_epsilon:.3f}")

    #å¦‚æœç•¶å‰ç´¯ç©çå‹µè¶…éæ­·å²æœ€ä½³å€¼ï¼Œä¿å­˜æ¨¡å‹çš„æ¬Šé‡ æ¯æ¬¡è¶…éæœ€ä½³å€¼å°±æœƒä¿ç•™ä¸€æ¬¡
    if cumulative_reward > best_reward:
        best_reward = cumulative_reward
        if EXTREME_MODE:
            os.makedirs("liang_test_extreme", exist_ok=True)
            #å‘½åé‚è¼¯æ˜¯æ¡ç¬¬å¹¾æ­¥+æœ€ä½³çå‹µ+è‡ªè¨‚ç¾©çå‹µçš„ç´¯ç©ç¸½åˆ
            model_path = os.path.join("liang_test_extreme",f"step_{timestep}_reward_{int(best_reward)}.pth")
            torch.save(dqn.q_net.state_dict(), model_path)
            print(f"Model saved: {model_path}")
        else:
            os.makedirs("liang_test", exist_ok=True)
            #å‘½åé‚è¼¯æ˜¯æ¡ç¬¬å¹¾æ­¥+æœ€ä½³çå‹µ+è‡ªè¨‚ç¾©çå‹µçš„ç´¯ç©ç¸½åˆ
            model_path = os.path.join("liang_test",f"step_{timestep}_reward_{int(best_reward)}.pth")
            torch.save(dqn.q_net.state_dict(), model_path)
            print(f"Model saved: {model_path}")

env.close()

# ========== é—œé–‰å½±ç‰‡éŒ„è£½ ===========
if video_writer is not None:
    video_writer.release()
    print(f"âœ… è¨“ç·´å½±ç‰‡å·²å„²å­˜: {VIDEO_OUTPUT_PATH}")
    print(f"   ç¸½å¹€æ•¸: {total_video_frames}")

