"""
PPO æ¨¡å‹è©•ä¼°è…³æœ¬
ç”¨æ–¼æ¸¬è©¦ PPO è¨“ç·´çš„æ¨¡å‹
"""
import os
import numpy as np
import torch
import torch.nn as nn
from torch.distributions import Categorical
import time

import gym
import gym_super_mario_bros
from gym.wrappers import StepAPICompatibility
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT

from utils import preprocess_frame

# ========== é…ç½® ===========
# MODEL_PATH = "./ckpt_ppo/ppo_best_avg_distance_1677_ep_2556.pth"  # PPO æ¨¡å‹è·¯å¾‘
MODEL_PATH = "/home/liang/Desktop/NCKU-CVDL-2025/SuperMario_SampleCode251218/ckpt_ppo/ppo_best_single_distance_1677_ep_2556.pth"

VISUALIZE = True
FRAME_DELAY = 0.02              # æ¯å¹€å»¶é² (0.02 = 50 FPS)
TOTAL_EPISODES = 10
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ========== Actor-Critic ç¶²è·¯ (å¿…é ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒ) ===========
class ActorCritic(nn.Module):
    """
    Actor-Critic ç¶²è·¯
    å…±äº«ç‰¹å¾µæå–å±¤ï¼Œåˆ†åˆ¥è¼¸å‡ºç­–ç•¥å’Œåƒ¹å€¼
    """
    def __init__(self, obs_shape, n_actions):
        super(ActorCritic, self).__init__()
        
        # å…±äº«å·ç©å±¤
        self.conv = nn.Sequential(
            nn.Conv2d(obs_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
        )
        
        # è¨ˆç®—å·ç©è¼¸å‡ºå¤§å°
        conv_out_size = self._get_conv_out(obs_shape)
        
        # å…±äº«å…¨é€£æ¥å±¤
        self.fc = nn.Sequential(
            nn.Linear(conv_out_size, 512),
            nn.ReLU(),
        )
        
        # Actor (ç­–ç•¥) è¼¸å‡º
        self.actor = nn.Linear(512, n_actions)
        
        # Critic (åƒ¹å€¼) è¼¸å‡º
        self.critic = nn.Linear(512, 1)
        
    def _get_conv_out(self, shape):
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))
    
    def forward(self, x):
        x = x / 255.0  # æ­£è¦åŒ–
        conv_out = self.conv(x)
        fc_out = self.fc(conv_out)
        
        # ç­–ç•¥ (log probabilities)
        policy = self.actor(fc_out)
        # åƒ¹å€¼
        value = self.critic(fc_out)
        
        return policy, value
    
    def get_action(self, state, deterministic=False):
        """é¸æ“‡å‹•ä½œ
        
        Args:
            state: è¼¸å…¥ç‹€æ…‹
            deterministic: æ˜¯å¦ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥ (é¸æ“‡æœ€é«˜æ¦‚ç‡çš„å‹•ä½œ)
        """
        policy, value = self.forward(state)
        probs = torch.softmax(policy, dim=-1)
        
        if deterministic:
            action = probs.argmax(dim=-1)
        else:
            dist = Categorical(probs)
            action = dist.sample()
            
        return action

# ========== ç’°å¢ƒè¨­ç½® ===========
def make_env():
    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')
    if isinstance(env, gym.wrappers.TimeLimit):
        env = env.env
    env = StepAPICompatibility(env, output_truncation_bool=False)
    env = JoypadSpace(env, SIMPLE_MOVEMENT)
    return env

# ========== è©•ä¼° ===========
def evaluate():
    print(f"ğŸ® Evaluating PPO model: {MODEL_PATH}")
    print(f"Using device: {device}")
    
    # ç’°å¢ƒ
    env = make_env()
    obs_shape = (1, 84, 84)
    n_actions = len(SIMPLE_MOVEMENT)
    
    # è¼‰å…¥æ¨¡å‹
    model = ActorCritic(obs_shape, n_actions).to(device)
    
    if os.path.exists(MODEL_PATH):
        try:
            model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
            model.eval()
            print(f"âœ… Model loaded successfully!")
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            return
    else:
        print(f"âŒ Model file not found: {MODEL_PATH}")
        return
    
    # è©•ä¼°
    episode_rewards = []
    episode_distances = []
    
    for episode in range(1, TOTAL_EPISODES + 1):
        state = env.reset()
        state = preprocess_frame(state)
        state = np.expand_dims(state, axis=0)
        
        done = False
        total_reward = 0
        max_x = 0
        
        while not done:
            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            
            with torch.no_grad():
                # ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥ï¼ˆé¸æ“‡æœ€é«˜æ¦‚ç‡çš„å‹•ä½œï¼‰
                action = model.get_action(state_tensor, deterministic=True)
            
            action_np = action.cpu().numpy()[0]
            next_state, reward, done, info = env.step(action_np)
            
            # é è™•ç†
            next_state = preprocess_frame(next_state)
            next_state = np.expand_dims(next_state, axis=0)
            
            total_reward += reward
            max_x = max(max_x, info['x_pos'])
            state = next_state
            
            if VISUALIZE:
                env.render()
                time.sleep(FRAME_DELAY)
        
        episode_rewards.append(total_reward)
        episode_distances.append(max_x)
        print(f"Episode {episode}/{TOTAL_EPISODES} - Reward: {total_reward:.0f} - Max X: {max_x}")
    
    env.close()
    
    # çµ±è¨ˆ
    print(f"\nğŸ“Š Evaluation Results:")
    print(f"Average Reward: {np.mean(episode_rewards):.1f}")
    print(f"Average Distance: {np.mean(episode_distances):.1f}")
    print(f"Best Distance: {max(episode_distances)}")
    print(f"Worst Distance: {min(episode_distances)}")

if __name__ == "__main__":
    evaluate()
