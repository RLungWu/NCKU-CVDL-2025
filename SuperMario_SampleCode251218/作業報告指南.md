# Super Mario RL 作業報告指南

## 老師的要求清單

| 項目 | 說明 | 
|------|------|
| 錄影講解 | 錄製螢幕，講解做了什麼、觀察到什麼 |
| Custom Reward 說明 | 解釋為什麼這樣定義獎勵函數 |
| 訓練時錄影 | 錄製訓練過程 |
| 測試權重錄影 | 用訓練好的模型進行測試並錄影 |

---

## 報告內容建議

### 做了什麼 (What I Did)

我在 `reward.py` 中實現了 **16 種獎勵/懲罰規則**，並支援兩種模式切換（Normal / Extreme）。
專注於**快速通關**目標，新增了 7 種速度和進度相關的獎勵函數。

#### 獎勵函數列表（共 16 種）

##### 原有獎勵函數（9 種）

| 編號 | 函數名稱 | 類型 | Normal | Extreme | 說明 |
|------|----------|------|--------|---------|------|
| 1 | `get_coin_reward` | 獎勵 | +8 | +20 | 收集硬幣 |
| 2 | `distance_y_offset_reward` | 獎勵 | +3/0 | +10/0 | 跳躍/下落 |
| 3 | `distance_x_offset_reward` | 獎勵/懲罰 | +30/-25 | +100/-80 | 前進/後退 |
| 4 | `monster_score_reward` | 獎勵 | +10 | +30 | 擊敗敵人 |
| 5 | `final_flag_reward` | 獎勵 | +1000 | +2000 | 到達終點 |
| 6 | `time_efficiency_reward` | 懲罰 | -8 | -15 | 時間消耗過多 |
| 7 | `death_penalty` | 懲罰 | -300 | -500 | 失去生命 |
| 8 | `stagnation_penalty` | 懲罰 | -20 | -50 | 原地不動 |
| 9 | `life_bonus_reward` | 獎勵 | +100 | +300 | 獲得1UP |

##### 新增獎勵函數（7 種 - 快速通關專用）

| 編號 | 函數名稱 | 類型 | Normal | Extreme | 說明 |
|------|----------|------|--------|---------|------|
| 10 | `speed_bonus_reward` | 獎勵 | +15 | +50 | 快速前進（跑動）|
| 11 | `momentum_bonus_reward` | 獎勵 | +20 | +60 | 連續前進 5 步以上 |
| 12 | `new_distance_record_reward` | 獎勵 | 動態 | 動態 | 達到最遠距離（每格 +0.5）|
| 13 | `obstacle_clear_reward` | 獎勵 | +25 | +80 | 成功越過障礙/敵人 |
| 14 | `progress_efficiency_reward` | 獎勵/懲罰 | +5/-10 | +5/-30 | 進度效率 |
| 15 | `precise_jump_reward` | 獎勵 | +5 | +5 | 精準跳躍 |
| 16 | `perfect_clear_bonus` | 獎勵 | 動態 | 動態 | 通關時剩餘時間（每秒 ×2）|

#### 訓練超參數設定 (run.py)

| 參數 | 數值 | 說明 |
|------|------|------|
| `LR` | 0.005 | 學習率 (加速收斂) |
| `BATCH_SIZE` | 64 | 批次大小 |
| `GAMMA` | 0.99 | 折扣因子 (重視長期獎勵) |
| `EPSILON_START` | 1.0 | 初始探索率 100% |
| `EPSILON_END` | 0.1 | 最終探索率 10% |
| `EPSILON_DECAY` | 0.995 | 每回合探索率衰減 |
| `TOTAL_TIMESTEPS` | 2000 | 總訓練回合數 |
| `FRAME_SKIP` | 2 | 跳幀數 (加速訓練) |

---

### 2️⃣ 為什麼這樣設計 (Why I Designed This Way)

```
【獎勵設計理念 - 快速通關優化】

核心獎勵 (最重要):
1. 前進獎勵 (+30/+100)：Mario 的主要目標是向右前進
2. 終點獎勵 (+1000/+2000)：通關是最終目標，給予最大獎勵
3. 速度獎勵 (+15/+50)：【新增】鼓勵快速移動（按住 B 跑動）

連續性獎勵 (維持動量):
4. 動量獎勵 (+20/+60)：【新增】連續前進 5 步以上額外獎勵
5. 距離紀錄 (動態)：【新增】到達新的最遠位置時獎勵

輔助獎勵:
6. 硬幣獎勵 (+8/+20)：收集硬幣是次要目標
7. 跳躍獎勵 (+3/+10)：適度鼓勵跳躍
8. 擊敗敵人 (+10/+30)：中等獎勵

【懲罰設計理念】

核心懲罰 (最重要):
1. 死亡懲罰 (-300/-500)：死亡是最嚴重的，必須強烈避免
2. 後退懲罰 (-25/-80)：阻止後退
3. 停滯懲罰 (-20/-50)：不能原地發呆

輕微懲罰:
4. 下落懲罰 (0)：設為 0！太高會不敢跳躍
5. 時間浪費 (-8/-15)：鼓勵快速通關
6. 進度太慢 (-10/-30)：【新增】落後於時間進度時懲罰

【為什麼新增 7 個函數？】
- 速度獎勵：原本只有前進獎勵，AI 可能慢慢走。新增速度獎勵後，AI 會學習跑動
- 動量獎勵：避免 AI 走走停停，維持連續運動
- 距離紀錄：激勵探索，到新區域就有獎勵
- 完美通關：時間剩越多獎勵越高，鼓勵快速過關
```

### 觀察到什麼 (What I Observed)

#### 訓練過程觀察

| 階段 | Epsilon | 行為 |
|------|---------|------|
| 初期 (1-100) | 1.0→0.6 | Agent 隨機行動，經常死亡，探索各種可能 |
| 中期 (100-500) | 0.6→0.1 | 開始學會向右移動，開始嘗試跳躍 |
| 後期 (500-2000) | 0.1 | 學會跑動和跳躍，可以越過障礙物 |

#### 評估結果比較

| 測試模型 | 總獎勵 | x_pos | Frames | 說明 |
|----------|--------|-------|--------|------|
| Extreme (step_1368) | 1,051 | 1,072 | 432 | 快速到達較遠距離後死亡 |
| Normal (step_1417) | 494 | 719 | 3,478 | 較緩慢但持續嘗試 |

#### 關鍵發現

1. **速度獎勵有效**：Extreme 模式的 speed_bonus (+50) 讓 AI 學會跑動
2. **x_pos 達到 1072**：成功越過第一個敵人
3. **Extreme 模式學得更快**：但穩定性較差，容易死亡


---


【做了什麼】
我設計了 16 種獎勵和懲罰規則來訓練 AI 玩 Mario。
其中 9 種是基本規則，另外 7 種是專門為「快速通關」目標新增的。

主要新增的獎勵包括：
- speed_bonus：獎勵 AI 快速跑動（按住 B 鍵）
- momentum_bonus：獎勵連續前進 5 步以上，維持動量
- perfect_clear_bonus：通關時剩餘時間越多，獎勵越高

【訓練設定】
- 使用 Epsilon Decay：從 100% 探索率逐漸降到 10%
- 學習率調整為 0.005，加速收斂
- Gamma = 0.99：重視長期獎勵
- 總共訓練 2000 回合

【為什麼這樣設計】
核心理念是「快速通關」：
1. 前進獎勵 (+100) 和速度獎勵 (+50) 讓 AI 學會跑動
2. 動量獎勵 (+60) 讓 AI 維持連續前進
3. 下落懲罰設為 0，避免 AI 不敢跳躍
4. 死亡懲罰 (-500) 足夠高，讓 AI 學會避險

【觀察到什麼】
使用 Extreme 模式訓練後：
- AI 的 x_pos 達到 1072，成功越過第一個敵人
- 速度獎勵讓 AI 學會用跑的而不是走的
- 動量獎勵讓 AI 保持連續前進，不會停停走走

【結論】
相比原本的 9 個獎勵函數，新增 7 個快速通關獎勵後：
- 學習速度更快
- 最終距離從約 200 提升到 1072
- 速度和動量獎勵是關鍵改進
```

---

## 結果數據整理

| 指標 | Normal 模式 | Extreme 模式 |
|------|-------------|--------------|
| 最佳獎勵 | 37,291 | 106,766 |
| 最遠距離 | 719 | 1,072 |
| 訓練步數 | 1,417 | 1,368 |
| 評估 Frames | 3,478 | 432 |

### 獎勵函數改進對比

| 版本 | 函數數量 | 最遠 x_pos | 說明 |
|------|----------|------------|------|
| 原始版 | 9 | ~200 | 基本規則 |
| 快速通關版 | 16 | 1,072 | 新增速度、動量獎勵 |
