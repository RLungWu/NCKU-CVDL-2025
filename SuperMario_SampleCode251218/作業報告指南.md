# 🎮 Super Mario RL 作業報告指南

## 📋 老師的要求清單

| 項目 | 說明 | 狀態 |
|------|------|------|
| 錄影講解 | 錄製螢幕，講解做了什麼、觀察到什麼 | ⬜ |
| Custom Reward 說明 | 解釋為什麼這樣定義獎勵函數 | ⬜ |
| 訓練時錄影 | 錄製訓練過程 | ⬜ |
| 測試權重錄影 | 用訓練好的模型進行測試並錄影 | ⬜ |

---

## 📝 報告內容建議

### 1️⃣ 做了什麼 (What I Did)

我在 `reward.py` 中實現了 **9 種獎勵/懲罰規則**，並支援兩種模式切換：

#### 獎勵函數列表

| 編號 | 函數名稱 | 類型 | Normal | Extreme | 說明 |
|------|----------|------|--------|---------|------|
| 1 | `get_coin_reward` | 獎勵 | +10 | +20 | 收集硬幣 |
| 2 | `distance_y_offset_reward` | 獎勵/懲罰 | +5/-5 | +10/-5 | 跳躍/下落 |
| 3 | `distance_x_offset_reward` | 獎勵/懲罰 | +25/-20 | +100/-80 | 前進/後退 |
| 4 | `monster_score_reward` | 獎勵 | +15 | +30 | 擊敗敵人 |
| 5 | `final_flag_reward` | 獎勵 | +500 | +2000 | 到達終點 |
| 6 | `time_efficiency_reward` | 懲罰 | -3 | -10 | 時間消耗過多 |
| 7 | `death_penalty` | 懲罰 | -200 | -500 | 失去生命 |
| 8 | `stagnation_penalty` | 懲罰 | -15 | -50 | 原地不動 |
| 9 | `life_bonus_reward` | 獎勵 | +100 | +300 | 獲得1UP |

#### 訓練超參數設定 (run.py)

| 參數 | 數值 | 說明 |
|------|------|------|
| `LR` | 0.00025 | 學習率 (DQN 經典值) |
| `BATCH_SIZE` | 64 | 批次大小 |
| `GAMMA` | 0.99 | 折扣因子 (重視長期獎勵) |
| `EPSILON_START` | 1.0 | 初始探索率 100% |
| `EPSILON_END` | 0.1 | 最終探索率 10% |
| `EPSILON_DECAY` | 0.995 | 每回合探索率衰減 |
| `TOTAL_TIMESTEPS` | 2000 | 總訓練回合數 |
| `FRAME_SKIP` | 2 | 跳幀數 (加速訓練) |

---

### 2️⃣ 為什麼這樣設計 (Why I Designed This Way)

```
【獎勵設計理念 - 策略性放大】

核心獎勵 (最重要):
1. 前進獎勵 (+25/+100)：Mario 的主要目標是向右前進，這是最重要的行為
2. 終點獎勵 (+500/+2000)：通關是最終目標，給予最大獎勵

輔助獎勵:
3. 硬幣獎勵 (+10/+20)：收集硬幣是次要目標
4. 跳躍獎勵 (+5/+10)：適度鼓勵跳躍，但不能太高避免亂跳
5. 擊敗敵人 (+15/+30)：中等獎勵，鼓勵但非必要

【懲罰設計理念】

核心懲罰 (最重要):
1. 死亡懲罰 (-200/-500)：死亡是最嚴重的，必須強烈避免
2. 後退懲罰 (-20/-80)：強烈阻止後退
3. 停滯懲罰 (-15/-50)：不能原地發呆

輕微懲罰:
4. 下落懲罰 (-5)：保持很低！太高會不敢跳躍
5. 時間浪費 (-3/-10)：輕微懲罰，鼓勵快速通關

【Epsilon Decay 設計】
- 初始探索率 100%：讓 AI 充分探索各種動作
- 逐漸衰減至 10%：隨訓練進行，開始利用學到的策略
- 衰減率 0.995：約在 500 回合後穩定
```

---

### 3️⃣ 觀察到什麼 (What I Observed)

訓練時觀察到的現象：

| 階段 | Epsilon | 行為 |
|------|---------|------|
| 初期 (1-100) | 1.0→0.6 | Agent 隨機行動，經常死亡，探索各種可能 |
| 中期 (100-500) | 0.6→0.1 | 開始學會向右移動，但常被第一個怪物殺死 |
| 後期 (500-2000) | 0.1 | 學會跳躍避開怪物，可以走得更遠 |

---

## 🎥 錄影步驟

### 步驟 1：訓練時錄影

```bash
# 先將 VISUALIZE 改為 True（在 run.py 第 53 行）
cd ~/Desktop/NCKU-CVDL-2025/SuperMario_SampleCode251218
python run.py
```

1. 開啟錄影軟體
2. 執行 `python run.py`
3. 錄製訓練過程（建議錄 5-10 分鐘）
4. 觀察 Epsilon 從 1.0 降到 0.1 的過程

### 步驟 2：測試權重錄影

```bash
# 修改 eval.py 中的 MODEL_PATH
# 例如：MODEL_PATH = os.path.join("liang_test", "step_xxx_reward_xxx.pth")
python eval.py
```

1. 開啟錄影軟體
2. 執行測試腳本
3. 錄製 Agent 的表現

---

## 🎤 講解腳本範例

```
大家好，這是我的 Super Mario 強化學習作業。

【做了什麼】
我設計了 9 種獎勵和懲罰規則來訓練 AI 玩 Mario。
並且實現了兩種模式：Normal Mode 和 Extreme Mode。
Extreme Mode 使用策略性放大，讓核心信號放大 4 倍，加速學習。

【訓練設定】
- 使用 Epsilon Decay：從 100% 探索率逐漸降到 10%
- Gamma = 0.99：讓 AI 更重視長期獎勵，學會跳過敵人
- Frame Skip = 2：加速訓練同時保持反應時間
- 總共訓練 2000 回合

【為什麼這樣設計】
主要的目標是讓 Mario 往右走並到達終點，
所以前進獎勵 (+100) 和通關獎勵 (+2000) 是最高的。
死亡懲罰 (-500) 也很高，避免 AI 學會送死。
特別注意下落懲罰只有 -5，因為太高會讓 AI 不敢跳躍。

【觀察到什麼】
訓練初期 Epsilon=1.0，AI 完全隨機探索，經常死亡。
訓練到 500 回合後，Epsilon 降到 0.1，AI 開始學會跳過障礙物。
最後 AI 可以走到 xxxx 的位置（根據實際結果填寫）。

【結論】
這個獎勵函數的設計讓 AI 在 2000 回合內學會了基本的前進和跳躍。
Epsilon Decay 的設計讓 AI 從探索轉向利用，是訓練成功的關鍵。
```
