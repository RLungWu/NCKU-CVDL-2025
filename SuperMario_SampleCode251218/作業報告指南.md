# 🎮 Super Mario RL 作業報告指南

## 📋 老師的要求清單

| 項目 | 說明 | 狀態 |
|------|------|------|
| 錄影講解 | 錄製螢幕，講解做了什麼、觀察到什麼 | ⬜ |
| Custom Reward 說明 | 解釋為什麼這樣定義獎勵函數 | ⬜ |
| 訓練時錄影 | 錄製訓練過程 | ⬜ |
| 測試權重錄影 | 用訓練好的模型進行測試並錄影 | ⬜ |

---

## 🎬 錄影方式

### Linux 推薦錄影工具：
```bash
# 安裝 OBS Studio（推薦）
sudo apt install obs-studio

# 或使用 SimpleScreenRecorder
sudo apt install simplescreenrecorder

# 或使用內建的 Kazam
sudo apt install kazam
```

---

## 📝 報告內容建議

### 1️⃣ 做了什麼 (What I Did)

我在 `reward.py` 中實現了 **9 種獎勵/懲罰規則**：

| 編號 | 函數名稱 | 類型 | 數值 | 說明 |
|------|----------|------|------|------|
| 1 | `get_coin_reward` | 獎勵 | +5 | 收集硬幣 |
| 2 | `distance_y_offset_reward` | 獎勵/懲罰 | +10/-10 | 跳躍/下落 |
| 3 | `distance_x_offset_reward` | 獎勵/懲罰 | +10/-10 | 前進/後退 |
| 4 | `monster_score_reward` | 獎勵/懲罰 | +8/-10 | 擊敗敵人/無得分 |
| 5 | `final_flag_reward` | 獎勵 | +100 | 到達終點 |
| 6 | `time_efficiency_reward` | 懲罰 | -5 | 時間消耗過多 |
| 7 | `death_penalty` | 懲罰 | -100 | 失去生命 |
| 8 | `stagnation_penalty` | 懲罰 | -5 | 原地不動 |
| 9 | `life_bonus_reward` | 獎勵 | +50 | 獲得1UP |

### 2️⃣ 為什麼這樣設計 (Why I Designed This Way)

```
【獎勵設計理念】

1. 前進獎勵 (+10)：Mario 的主要目標是向右前進，這是最重要的行為
2. 硬幣獎勵 (+5)：收集硬幣是次要目標，獎勵較小避免分心
3. 終點獎勵 (+100)：通關是最終目標，給予最大獎勵
4. 跳躍獎勵 (+10)：鼓勵跳躍來越過障礙物
5. 擊敗敵人 (+8)：鼓勵消滅敵人而非躲避

【懲罰設計理念】

1. 死亡懲罰 (-100)：死亡是最嚴重的，與終點獎勵相抵消
2. 後退懲罰 (-10)：避免 AI 學會後退逃跑
3. 停滯懲罰 (-5)：避免 AI 原地不動
4. 時間浪費 (-5)：鼓勵快速通關
```

### 3️⃣ 觀察到什麼 (What I Observed)

訓練時可能觀察到的現象：
- 初期：Agent 隨機行動，經常死亡
- 中期：開始學會向右移動，但常被第一個怪物殺死
- 後期：學會跳躍和避開怪物，可以走得更遠

---

## 🎥 錄影步驟

### 步驟 1：訓練時錄影

```bash
# 確保 VISUALIZE = True
cd ~/Desktop/NCKU-CVDL-2025/SuperMario_SampleCode251218
python run.py
```

1. 開啟錄影軟體
2. 執行 `python run.py`
3. 錄製訓練過程（建議錄 5-10 分鐘）
4. 觀察 Agent 的學習進度

### 步驟 2：測試權重錄影

```bash
# 使用 eval.py 測試訓練好的模型
python eval.py --model_path ckpt_test/你的最佳模型.pth
```

1. 開啟錄影軟體
2. 執行測試腳本
3. 錄製 Agent 的表現

---

## 🎤 講解腳本範例

```
大家好，這是我的 Super Mario 強化學習作業。

【做了什麼】
我設計了 9 種獎勵和懲罰規則來訓練 AI 玩 Mario。
包括收集硬幣獎勵、前進獎勵、死亡懲罰等等。

【為什麼這樣設計】
主要的目標是讓 Mario 往右走並到達終點，
所以前進和通關的獎勵最高。
死亡懲罰也很高，避免 AI 學會送死。

【觀察到什麼】
訓練初期 AI 會到處亂跑，經常掉進洞裡。
訓練一段時間後，AI 開始學會跳過障礙物。
最後 AI 可以走到 xxxx 的位置（根據實際結果填寫）。

【結論】
這個獎勵函數的設計讓 AI 在 1000 回合內學會了基本的前進和跳躍。
```
