"""
çå‹µå‡½æ•¸è¶…åƒæ•¸æœç´¢
è‡ªå‹•å˜—è©¦ä¸åŒçš„çå‹µé…ç½®ï¼Œæ‰¾åˆ°æœ€ä½³çµ„åˆ
"""
import os
import json
import numpy as np
import torch
from datetime import datetime
from itertools import product
import random

import gym
import gym_super_mario_bros
from gym.wrappers import StepAPICompatibility
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT

from utils import preprocess_frame
from model import CustomCNN
from DQN import DQN, ReplayMemory

# ========== æœç´¢é…ç½® ===========
SEARCH_METHOD = "random"  # "grid", "random", "manual"
NUM_RANDOM_TRIALS = 10    # éš¨æ©Ÿæœç´¢çš„è©¦é©—æ¬¡æ•¸
EPISODES_PER_TRIAL = 100  # æ¯å€‹é…ç½®è¨“ç·´çš„ episode æ•¸
EVAL_EPISODES = 10        # è©•ä¼°æ™‚çš„ episode æ•¸

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ========== è¶…åƒæ•¸æœç´¢ç©ºé–“ ===========
HYPERPARAMETER_SPACE = {
    # åŸºæœ¬çå‹µ
    'coin_reward': [5, 10, 20],
    'forward_reward': [0.5, 1.0, 2.0],
    'backward_penalty': [-10, -5, -1],
    
    # æ•µäººç›¸é—œ
    'kill_base_reward': [10, 20, 50],
    'enemy_approach_penalty': [-10, -5, -2],
    
    # å‘æ´ç›¸é—œ
    'hole_crossed_reward': [50, 100, 200],
    'fall_death_penalty': [-300, -200, -100],
    
    # çªç ´çå‹µ
    'breakthrough_large_reward': [1.0, 2.0, 5.0],
    'stagnation_penalty': [-5, -2, -1],
}

# ========== é è¨­é…ç½® (åŸºæº–) ===========
DEFAULT_CONFIG = {
    'coin_reward': 10,
    'forward_reward': 1.0,
    'backward_penalty': 0.01,
    'jump_reward': 0.5,
    'score_reward_multiplier': 0.1,
    'flag_reward': 1000,
    'enemy_approach_penalty': -5,
    'enemy_jump_over_reward': 20,
    'enemy_safe_distance_reward': 1,
    'kill_base_reward': 20,
    'kill_combo_bonus': 10,
    'kill_per_enemy_bonus': 10,
    'stomp_kill_bonus': 5,
    'survival_reward': 0.3,
    'death_penalty': -100,
    'life_lost_penalty': -100,
    'fast_forward_reward': 0.5,
    'efficiency_reward': 2,
    'threat_jump_reward': 10,
    'unnecessary_jump_penalty': -0.1,
    'powerup_reward': 50,
    'oneup_reward': 200,
    'breakthrough_small_reward': 0.5,
    'breakthrough_large_reward': 2.0,
    'stagnation_penalty': -1,
    'jump_at_frontier_reward': 5,
    'forward_jump_reward': 3,
    'hole_approach_jump_reward': 15,
    'hole_approach_no_jump_penalty': -2,
    'hole_over_reward': 20,
    'hole_crossed_reward': 100,
    'falling_penalty': -30,
    'fall_death_penalty': -200,
    'air_forward_reward': 0.5,
}

# ========== ç’°å¢ƒå»ºç«‹ ===========
def make_env():
    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')
    if isinstance(env, gym.wrappers.TimeLimit):
        env = env.env
    env = StepAPICompatibility(env, output_truncation_bool=False)
    env = JoypadSpace(env, SIMPLE_MOVEMENT)
    return env

# ========== å¿«é€Ÿè¨“ç·´è©•ä¼° ===========
def quick_train_and_eval(config, num_episodes=100, eval_episodes=10):
    """
    å¿«é€Ÿè¨“ç·´ä¸¦è©•ä¼°ä¸€å€‹é…ç½®
    è¿”å›å¹³å‡è·é›¢å’Œæœ€å¤§è·é›¢
    """
    # æ›´æ–°å…¨å±€çå‹µé…ç½®
    import reward
    for key, value in config.items():
        if key in reward.REWARD_CONFIG:
            reward.REWARD_CONFIG[key] = value
    
    # ç’°å¢ƒ
    env = make_env()
    obs_shape = (1, 84, 84)
    n_actions = len(SIMPLE_MOVEMENT)
    
    # DQN
    dqn = DQN(
        model=CustomCNN,
        state_dim=obs_shape,
        action_dim=n_actions,
        learning_rate=0.0001,
        gamma=0.99,
        epsilon=1.0,
        target_update=50,
        device=device
    )
    
    memory = ReplayMemory(10000)
    
    distances = []
    epsilon = 1.0
    
    for episode in range(num_episodes):
        state = env.reset()
        state = preprocess_frame(state)
        state = np.expand_dims(state, axis=0)
        
        prev_info = {"x_pos": 40, "y_pos": 0, "score": 0, "coins": 0, 
                     "time": 400, "flag_get": False, "life": 3}
        
        done = False
        max_x = 0
        
        # é‡ç½®è¿½è¹¤è®Šæ•¸
        reward.reset_max_x()
        try:
            reward.reset_enemy_tracking()
            reward.reset_hole_tracking()
        except:
            pass
        
        while not done:
            # é¸æ“‡å‹•ä½œ
            if np.random.rand() < epsilon:
                action = np.random.randint(n_actions)
            else:
                with torch.no_grad():
                    state_tensor = torch.tensor([state], dtype=torch.float32, device=device)
                    q_values = dqn.q_net(state_tensor)
                    action = q_values.argmax(dim=1).item()
            
            # åŸ·è¡Œå‹•ä½œ
            next_state, base_reward, done, info = env.step(action)
            
            # è¨ˆç®—çå‹µ
            custom_reward = reward.calculate_smart_reward(env, info, base_reward, prev_info)
            
            # é è™•ç†
            next_state_processed = preprocess_frame(next_state)
            next_state_processed = np.expand_dims(next_state_processed, axis=0)
            
            # å­˜å…¥è¨˜æ†¶
            memory.push(state, action, custom_reward, next_state_processed, done)
            
            # è¨“ç·´
            if len(memory) >= 32:
                batch = memory.sample(32)
                state_dict = {
                    'states': batch[0],
                    'actions': batch[1],
                    'rewards': batch[2],
                    'next_states': batch[3],
                    'dones': batch[4],
                }
                dqn.train_per_step(state_dict)
            
            state = next_state_processed
            prev_info = info
            max_x = max(max_x, info['x_pos'])
        
        distances.append(max_x)
        epsilon = max(0.1, epsilon * 0.995)
    
    env.close()
    
    # è©•ä¼°
    avg_distance = np.mean(distances[-eval_episodes:])
    max_distance = max(distances)
    
    return {
        'avg_distance': avg_distance,
        'max_distance': max_distance,
        'all_distances': distances,
    }

# ========== Grid Search ===========
def grid_search():
    """ç¶²æ ¼æœç´¢æ‰€æœ‰è¶…åƒæ•¸çµ„åˆ"""
    results = []
    
    # åªæœç´¢éƒ¨åˆ†é—œéµåƒæ•¸ï¼ˆå…¨æœç´¢å¤ªæ…¢ï¼‰
    key_params = ['forward_reward', 'kill_base_reward', 'hole_crossed_reward']
    
    combinations = list(product(*[HYPERPARAMETER_SPACE[k] for k in key_params]))
    
    print(f"Grid Search: {len(combinations)} combinations")
    
    for i, combo in enumerate(combinations):
        config = DEFAULT_CONFIG.copy()
        for k, v in zip(key_params, combo):
            config[k] = v
        
        print(f"\n[{i+1}/{len(combinations)}] Testing: {dict(zip(key_params, combo))}")
        
        result = quick_train_and_eval(config, EPISODES_PER_TRIAL, EVAL_EPISODES)
        result['config'] = {k: v for k, v in zip(key_params, combo)}
        results.append(result)
        
        print(f"  Avg Distance: {result['avg_distance']:.1f}, Max: {result['max_distance']}")
    
    return results

# ========== Random Search ===========
def random_search(num_trials=10):
    """éš¨æ©Ÿæœç´¢è¶…åƒæ•¸"""
    results = []
    
    print(f"Random Search: {num_trials} trials")
    
    for i in range(num_trials):
        config = DEFAULT_CONFIG.copy()
        
        # éš¨æ©Ÿé¸æ“‡éƒ¨åˆ†åƒæ•¸
        sampled = {}
        for key, values in HYPERPARAMETER_SPACE.items():
            if random.random() < 0.5:  # 50% æ©Ÿç‡èª¿æ•´é€™å€‹åƒæ•¸
                config[key] = random.choice(values)
                sampled[key] = config[key]
        
        print(f"\n[{i+1}/{num_trials}] Testing: {sampled}")
        
        result = quick_train_and_eval(config, EPISODES_PER_TRIAL, EVAL_EPISODES)
        result['config'] = sampled
        results.append(result)
        
        print(f"  Avg Distance: {result['avg_distance']:.1f}, Max: {result['max_distance']}")
    
    return results

# ========== æ‰‹å‹•èª¿æ•´å»ºè­° ===========
def manual_tuning_guide():
    """è¼¸å‡ºæ‰‹å‹•èª¿æ•´çš„å»ºè­°"""
    guide = """
    ========================================
    ğŸ¯ çå‹µè¶…åƒæ•¸æ‰‹å‹•èª¿æ•´æŒ‡å—
    ========================================
    
    1. å¦‚æœ Mario ä¸é¡˜æ„å‘å‰èµ°:
       â†’ å¢åŠ  forward_reward (1.0 â†’ 2.0)
       â†’ æ¸›å°‘ backward_penalty (-5 â†’ -10)
    
    2. å¦‚æœ Mario ä¸é¡˜æ„è·³èº:
       â†’ å¢åŠ  jump_reward (0.5 â†’ 1.0)
       â†’ å¢åŠ  threat_jump_reward (10 â†’ 20)
    
    3. å¦‚æœ Mario ç¸½æ˜¯è¢«æ•µäººæ®ºæ­»:
       â†’ å¢åŠ  enemy_approach_penalty (-5 â†’ -10)
       â†’ å¢åŠ  kill_base_reward (20 â†’ 50)
    
    4. å¦‚æœ Mario æ‰å…¥å‘æ´:
       â†’ å¢åŠ  hole_crossed_reward (100 â†’ 200)
       â†’ å¢åŠ  fall_death_penalty (-200 â†’ -300)
       â†’ å¢åŠ  hole_approach_jump_reward (15 â†’ 30)
    
    5. å¦‚æœ Mario å¡åœ¨æŸå€‹åœ°æ–¹:
       â†’ å¢åŠ  stagnation_penalty (-1 â†’ -5)
       â†’ å¢åŠ  breakthrough_large_reward (2.0 â†’ 5.0)
    
    6. å¦‚æœ Mario è·³å¤ªå¤šæ¬¡:
       â†’ å¢åŠ  unnecessary_jump_penalty (-0.1 â†’ -1)
    
    ========================================
    """
    print(guide)
    return guide

# ========== ä¿å­˜çµæœ ===========
def save_results(results, filename=None):
    """ä¿å­˜æœç´¢çµæœ"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"hyperparam_search_{timestamp}.json"
    
    # è½‰æ›ç‚ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable = []
    for r in results:
        s = {
            'config': r['config'],
            'avg_distance': float(r['avg_distance']),
            'max_distance': int(r['max_distance']),
        }
        serializable.append(s)
    
    # æ’åº
    serializable.sort(key=lambda x: x['avg_distance'], reverse=True)
    
    # ä¿å­˜
    os.makedirs("hyperparam_results", exist_ok=True)
    filepath = os.path.join("hyperparam_results", filename)
    
    with open(filepath, 'w') as f:
        json.dump(serializable, f, indent=2)
    
    print(f"\nğŸ“ Results saved to {filepath}")
    
    # è¼¸å‡ºæœ€ä½³é…ç½®
    print("\nğŸ† Best Configurations:")
    for i, s in enumerate(serializable[:3]):
        print(f"  {i+1}. Avg: {s['avg_distance']:.1f}, Max: {s['max_distance']}")
        print(f"     Config: {s['config']}")
    
    return filepath

# ========== ä¸»å‡½æ•¸ ===========
def main():
    print("=" * 50)
    print("ğŸ” Reward Hyperparameter Search")
    print("=" * 50)
    
    if SEARCH_METHOD == "grid":
        results = grid_search()
    elif SEARCH_METHOD == "random":
        results = random_search(NUM_RANDOM_TRIALS)
    elif SEARCH_METHOD == "manual":
        manual_tuning_guide()
        return
    else:
        print(f"Unknown method: {SEARCH_METHOD}")
        return
    
    save_results(results)
    
    print("\nâœ… Search complete!")

if __name__ == "__main__":
    main()
